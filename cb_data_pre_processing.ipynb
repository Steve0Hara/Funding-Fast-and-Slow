{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# Crunchbase Study Dataset: Pre-processing\n",
    "\n",
    "This notebook takes a merged Crunchbase export (one row per organization) and produces cleaned, analysis-ready CSVs for the thesis study.\n",
    "\n",
    "**Inputs**\n",
    "- `merged_big_30_11.csv` (merged Crunchbase dataset)\n",
    "\n",
    "**Outputs**\n",
    "- `global_companies.csv` (all retained orgs)\n",
    "- `uk_companies.csv` (subset where `org_country == 'GBR'`)\n",
    "- `usa_companies.csv` (subset where `org_country == 'USA'`)\n",
    "\n",
    "**What happens in this pipeline (high-level)**\n",
    "1. Load the merged Crunchbase file and de-duplicate by `org_uuid`.\n",
    "2. Apply row-level cleaning rules (status cleanup, optional backfilling, dropping known-bad rows).\n",
    "3. Remove impossible timelines (e.g., `closed_on < founded_on`).\n",
    "4. Enforce a study *data freeze* (`freeze_date`) and limit companies to a founded-year window.\n",
    "5. Keep only the columns needed downstream.\n",
    "6. Parse the raw funding-round strings into stage-specific date/amount/UUID columns.\n",
    "7. Add a coarse `region` variable from ISO country codes.\n",
    "8. Export global + UK + US subsets.\n",
    "\n",
    "Notes:\n",
    "- The notebook uses **absolute paths** for input/output; adjust them if you move the project directory.\n",
    "- Several steps are controlled via `STUDY_PARAMS['preprocessing']` for reproducibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ee7a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports used across the notebook.\n",
    "# - pandas: core DataFrame wrangling\n",
    "# - pycountry_convert: ISO country → continent mapping for a coarse `region` feature\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pycountry_convert as pc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0b5fba",
   "metadata": {},
   "source": [
    "# Setting Parameters\n",
    "\n",
    "This section defines the parameters that govern the preprocessing logic:\n",
    "- `STUDY_PARAMS['preprocessing']`: toggles and cutoffs (e.g., date freeze, founded-year window).\n",
    "- File paths for the merged input dataset and the exported outputs.\n",
    "\n",
    "Keeping these values centralized makes the preprocessing reproducible and easier to audit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c82d82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Study configuration (single source of truth).\n",
    "# Adjust values here to re-run the pipeline with different cutoffs/toggles.\n",
    "#\n",
    "# Key parameters:\n",
    "# - backfill_closed_on: if True, imputes missing `closed_on` for closed companies.\n",
    "# - founded_year_range: defines the study cohort window.\n",
    "# - freeze_date: caps event dates to prevent leakage beyond the cut-off.\n",
    "\n",
    "# Centralized study parameters for easy tweaking\n",
    "STUDY_PARAMS = {\n",
    "    \"preprocessing\": {\n",
    "        \"backfill_closed_on\": False,  # Changed from False to True\n",
    "        \"drop_primary_role_investor\": True,\n",
    "        \"founded_year_range\": (2007, 2017),\n",
    "        # \"founded_year_range\": (2007, 2024),\n",
    "        \"freeze_date\": pd.Timestamp(\"2024-12-31\"),\n",
    "        \"stale_active_cutoff\": pd.Timestamp(\"2021-01-01\"), # deactivated\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b9755ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input/output locations.\n",
    "# Using absolute paths keeps this notebook unambiguous on the author machine;\n",
    "# if you move the repo, update these paths accordingly.\n",
    "\n",
    "data_path = Path(\"/Users/stefan/Desktop/Thesis/v4/Crunchbase Data/Merging CB Datasets/merged_big_30_11.csv\")\n",
    "output_path_global = \"/Users/stefan/Desktop/Thesis/v4/Study/cb data pre-processing/global_companies.csv\"\n",
    "output_path_uk = \"/Users/stefan/Desktop/Thesis/v4/Study/cb data pre-processing/uk_companies.csv\"\n",
    "output_path_usa =  \"/Users/stefan/Desktop/Thesis/v4/Study/cb data pre-processing/usa_companies.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6444a2",
   "metadata": {},
   "source": [
    "# Filtering Data\n",
    "\n",
    "Load the merged Crunchbase dataset and apply *row-level* cleaning rules. The goal is to remove duplicates and drop records that are clearly inconsistent (e.g., missing required identifiers or impossible event timelines).\n",
    "\n",
    "Most filtering steps preserve the original columns; structural reshaping happens later (funding-round parsing).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "049e0742",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the merged Crunchbase dataset.\n",
    "# `convert_dtypes()` gives pandas' best-guess nullable dtypes for cleaner downstream handling.\n",
    "\n",
    "df_crunchbase = pd.read_csv(data_path, low_memory=False).convert_dtypes()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13da2a1",
   "metadata": {},
   "source": [
    "### Removing Duplicate Orgs\n",
    "\n",
    "Crunchbase exports can contain duplicate rows for the same organization. We treat `org_uuid` as the unique identifier and keep the first occurrence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4239f75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 2,318 duplicate org_uuid rows\n"
     ]
    }
   ],
   "source": [
    "# De-duplicate organizations by `org_uuid` (unique org identifier).\n",
    "# Keeps the first occurrence and reports how many rows were removed.\n",
    "\n",
    "rows_before = len(df_crunchbase)\n",
    "df_crunchbase = df_crunchbase.dropna(subset=[\"org_uuid\"]).drop_duplicates(\"org_uuid\")\n",
    "removed = rows_before - len(df_crunchbase)\n",
    "print(f\"Removed {removed:,} duplicate org_uuid rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d4a521",
   "metadata": {},
   "source": [
    "## Filter\n",
    "\n",
    "From here on, the notebook applies a sequence of deterministic filters:\n",
    "- Optional `closed_on` backfilling for closed companies.\n",
    "- Status normalization/flagging (informational).\n",
    "- Dropping rows based on study rules (e.g., removing investors).\n",
    "- Removing exact duplicate rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "945a0d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read preprocessing config and create reusable masks.\n",
    "# Also prepares a boolean flag column to track whether `closed_on` was backfilled.\n",
    "\n",
    "preproc_cfg = STUDY_PARAMS[\"preprocessing\"]\n",
    "\n",
    "status_clean = df_crunchbase[\"status\"].astype(str).str.strip()\n",
    "closed_mask = status_clean.eq(\"closed\")\n",
    "closed_missing_before = closed_mask & df_crunchbase[\"closed_on\"].isna()\n",
    "total_closed = int(closed_mask.sum())\n",
    "\n",
    "df_crunchbase[\"closed_on_backfilled\"] = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2393fca",
   "metadata": {},
   "source": [
    "### Backfill missing `closed_on` dates (optional) and log coverage\n",
    "\n",
    "Some companies are labeled as `closed` but have missing `closed_on`. If `backfill_closed_on` is enabled, the notebook:\n",
    "1. Extracts the **last funding date** from the raw `funding_round_*` strings.\n",
    "2. Adds a 24-month runway assumption.\n",
    "3. Caps the imputed date at the study `freeze_date`.\n",
    "\n",
    "The notebook always logs how many rows are affected; the backfill itself is controlled by `STUDY_PARAMS`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a87676f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closed companies total: 176,004\n",
      "Closed companies missing closed_on: 112,350\n",
      "Closed companies missing closed_on (backfilled): 0\n",
      "Share of closed companies backfilled: 0.00%\n"
     ]
    }
   ],
   "source": [
    "# Optional: backfill missing `closed_on` for closed companies.\n",
    "# Strategy (if enabled): last funding date + 24 months, capped at freeze_date.\n",
    "\n",
    "import re\n",
    "\n",
    "# --- FIX: Ensure closed_on is datetime before assignment ---\n",
    "df_crunchbase[\"closed_on\"] = pd.to_datetime(df_crunchbase[\"closed_on\"], errors=\"coerce\")\n",
    "\n",
    "missing_closed_on_count = int(closed_missing_before.sum())\n",
    "filled_closed_on_count = 0\n",
    "\n",
    "if preproc_cfg[\"backfill_closed_on\"]:\n",
    "    print(\"Extracting last funding date from round details...\")\n",
    "    \n",
    "    # 1. Select the rows we need to fix\n",
    "    # We work on a copy to avoid SettingWithCopy warnings\n",
    "    target_rows = df_crunchbase.loc[closed_missing_before].copy()\n",
    "    \n",
    "    # 2. Define the columns to check (1 to 15 as requested)\n",
    "    round_cols = [f\"funding_round_{i}\" for i in range(1, 16)]\n",
    "    \n",
    "    # 3. Extract dates from each column\n",
    "    extracted_dates = pd.DataFrame(index=target_rows.index)\n",
    "    \n",
    "    for col in round_cols:\n",
    "        if col in target_rows.columns:\n",
    "            # Extract the date part\n",
    "            dates_str = target_rows[col].astype(str).str.extract(r'\\((\\d{4}-\\d{2}-\\d{2})\\)')[0]\n",
    "            extracted_dates[col] = pd.to_datetime(dates_str, errors='coerce')\n",
    "            \n",
    "    # 4. Find the MAX date across all 15 columns for each company\n",
    "    last_funding_computed = extracted_dates.max(axis=1)\n",
    "    \n",
    "    # 5. Apply the \"Runway\" Assumption (Last Funding + 24 Months)\n",
    "    imputed_closure = last_funding_computed + pd.DateOffset(months=24)\n",
    "    \n",
    "    # 6. Cap at Freeze Date\n",
    "    freeze_date = preproc_cfg[\"freeze_date\"]\n",
    "    capped_backfill = imputed_closure.where(\n",
    "        imputed_closure <= freeze_date,\n",
    "        freeze_date,\n",
    "    )\n",
    "    \n",
    "    # 7. Apply the backfill\n",
    "    # Only fill where we successfully found a funding date\n",
    "    valid_fill_mask = capped_backfill.notna()\n",
    "    \n",
    "    # Update the main DataFrame\n",
    "    # Align indices to ensure correct assignment\n",
    "    # We use reindex to align the boolean mask with the main dataframe's index\n",
    "    indexer = closed_missing_before & valid_fill_mask.reindex(df_crunchbase.index, fill_value=False)\n",
    "    \n",
    "    df_crunchbase.loc[indexer, \"closed_on\"] = capped_backfill\n",
    "    df_crunchbase.loc[indexer, \"closed_on_backfilled\"] = True\n",
    "    \n",
    "    filled_closed_on_count = int(valid_fill_mask.sum())\n",
    "\n",
    "percent_backfilled = (filled_closed_on_count / total_closed * 100.0) if total_closed else 0.0\n",
    "print(\n",
    "    f\"Closed companies total: {total_closed:,}\\n\"\n",
    "    f\"Closed companies missing closed_on: {missing_closed_on_count:,}\\n\"\n",
    "    f\"Closed companies missing closed_on (backfilled): {filled_closed_on_count:,}\\n\"\n",
    "    f\"Share of closed companies backfilled: {percent_backfilled:.2f}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2053137f",
   "metadata": {},
   "source": [
    "### Flag stale “active” companies and acquired records (informational)\n",
    "\n",
    "This step updates the `status` label for two cases:\n",
    "- Organizations still marked as `active` but not updated since `stale_active_cutoff`.\n",
    "- Organizations with a non-empty `acquired_by` field (treated as `acquired`).\n",
    "\n",
    "This is meant for downstream interpretability; it does **not** drop rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce75cf1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Flagged stale actives (no update since 2021-01-01): 0\n",
      "Flagged actives with acquisitions as acquired: 0\n"
     ]
    }
   ],
   "source": [
    "# Informational status relabeling.\n",
    "# - Marks stale 'active' orgs (no update since cutoff)\n",
    "# - Marks 'active' orgs with an acquirer as 'acquired'\n",
    "\n",
    "status_lower = status_clean.str.lower()\n",
    "org_updated_ts = pd.to_datetime(df_crunchbase[\"org_updated_at\"], errors=\"coerce\")\n",
    "stale_active_cutoff = preproc_cfg[\"stale_active_cutoff\"]\n",
    "\n",
    "active_mask = status_lower.eq(\"active\")\n",
    "stale_active_mask = active_mask & (org_updated_ts < stale_active_cutoff)\n",
    "acquired_flag_mask = active_mask & df_crunchbase[\"acquired_by\"].astype(str).str.strip().ne(\"\")\n",
    "\n",
    "df_crunchbase.loc[stale_active_mask, \"status\"] = \"listed as active but no update in 5 years\"\n",
    "df_crunchbase.loc[acquired_flag_mask, \"status\"] = \"acquired\"\n",
    "\n",
    "print(f\"Flagged stale actives (no update since {stale_active_cutoff.date()}): {int(stale_active_mask.sum()):,}\")\n",
    "print(f\"Flagged actives with acquisitions as acquired: {int(acquired_flag_mask.sum()):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0e73ae",
   "metadata": {},
   "source": [
    "### Drop unwanted rows and duplicates\n",
    "\n",
    "Apply the main study exclusion rules:\n",
    "- Optionally remove rows where `primary_role == 'investor'`.\n",
    "- Drop rows where an acquisition is indicated (`acquired_by` present) but no acquisition date is available.\n",
    "- Remove exact duplicate rows after filtering.\n",
    "\n",
    "The notebook prints a breakdown of removals for auditability.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a0eed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build exclusion masks used to drop rows:\n",
    "# - investor-only records (optional)\n",
    "# - acquired_by present but missing acquisition date\n",
    "# - closed but missing closed_on (tracked for reporting)\n",
    "\n",
    "primary_role = df_crunchbase[\"primary_role\"].astype(str).str.strip()\n",
    "mask_primary_role_investor = primary_role.eq(\"investor\")\n",
    "\n",
    "has_acquirer = (\n",
    "    df_crunchbase[\"acquired_by\"]\n",
    "    .fillna(\"\")        # keep real nulls empty\n",
    "    .astype(str)\n",
    "    .str.strip()\n",
    ")\n",
    "mask_acquired_mismatch = has_acquirer.ne(\"\") & df_crunchbase[\"acquired_on_first\"].isna()\n",
    "\n",
    "mask_acquired_mismatch = has_acquirer.ne(\"\") & df_crunchbase[\"acquired_on_first\"].isna()\n",
    "\n",
    "mask_closed_missing_date = status_clean.eq(\"closed\") & df_crunchbase[\"closed_on\"].isna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5ba8de1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the exclusion rules and remove exact duplicate rows post-filtering.\n",
    "\n",
    "drop_mask = mask_acquired_mismatch.copy()\n",
    "if preproc_cfg[\"drop_primary_role_investor\"]:\n",
    "    drop_mask |= mask_primary_role_investor\n",
    "\n",
    "dropped_by_rules = int(drop_mask.sum())\n",
    "df_filtered = df_crunchbase.loc[~drop_mask].copy()\n",
    "\n",
    "duplicate_mask = df_filtered.duplicated(keep=\"first\")\n",
    "duplicates_dropped = int(duplicate_mask.sum())\n",
    "if duplicates_dropped:\n",
    "    df_filtered = df_filtered.loc[~duplicate_mask].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fb13829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows filtered by reason:\n",
      "  primary_role == 'investor': 104,607\n",
      "  acquired_by present but acquired_on_first missing: 0\n",
      "  status == 'closed' but closed_on missing: 112,350\n",
      "  closed_on backfilled from org_updated_at: 0\n",
      "  exact duplicate rows: 0\n",
      "Total rows removed (union reasons + duplicates): 104,607\n",
      "Remaining rows: 3,887,655\n"
     ]
    }
   ],
   "source": [
    "# Audit printout: how many rows were filtered by each rule.\n",
    "\n",
    "counts = [\n",
    "    (\"acquired_by present but acquired_on_first missing\", int(mask_acquired_mismatch.sum())),\n",
    "    (\"status == 'closed' but closed_on missing\", int(mask_closed_missing_date.sum())),\n",
    "    (\"closed_on backfilled from org_updated_at\", filled_closed_on_count),\n",
    "    (\"exact duplicate rows\", duplicates_dropped),\n",
    "]\n",
    "if preproc_cfg[\"drop_primary_role_investor\"]:\n",
    "    counts.insert(0, (\"primary_role == 'investor'\", int(mask_primary_role_investor.sum())))\n",
    "\n",
    "print(\"Rows filtered by reason:\")\n",
    "for reason, count in counts:\n",
    "    print(f\"  {reason}: {count:,}\")\n",
    "\n",
    "total_removed = dropped_by_rules + duplicates_dropped\n",
    "print(f\"Total rows removed (union reasons + duplicates): {total_removed:,}\")\n",
    "print(f\"Remaining rows: {len(df_filtered):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb57a55",
   "metadata": {},
   "source": [
    "### Filter out negative event durations\n",
    "\n",
    "Remove organizations with impossible timelines:\n",
    "- `closed_on < founded_on`\n",
    "- `went_public_on < founded_on`\n",
    "- `acquired_on_first < founded_on`\n",
    "\n",
    "The notebook reports counts per failure type and then drops all flagged rows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "adc06df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_to_close: 44 negative durations\n",
      "time_to_ipo: 676 negative durations\n",
      "time_to_acquisition: 584 negative durations\n"
     ]
    }
   ],
   "source": [
    "# Helper: identify organizations with impossible event ordering (negative durations).\n",
    "# Used for reporting; the next cell performs the actual dropping.\n",
    "\n",
    "def find_negative_durations(df: pd.DataFrame) -> dict[str, pd.Index]:\n",
    "    required = [\"founded_on\", \"closed_on\", \"went_public_on\", \"acquired_on_first\"]\n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    founded = pd.to_datetime(df[\"founded_on\"], errors=\"coerce\")\n",
    "    closed = pd.to_datetime(df[\"closed_on\"], errors=\"coerce\")\n",
    "    went_public = pd.to_datetime(df[\"went_public_on\"], errors=\"coerce\")\n",
    "    acquired = pd.to_datetime(df[\"acquired_on_first\"], errors=\"coerce\")\n",
    "\n",
    "    masks = {\n",
    "        \"time_to_close\": founded.notna() & closed.notna() & (closed < founded),\n",
    "        \"time_to_ipo\": founded.notna() & went_public.notna() & (went_public < founded),\n",
    "        \"time_to_acquisition\": founded.notna() & acquired.notna() & (acquired < founded),\n",
    "    }\n",
    "\n",
    "    negatives = {\n",
    "        label: (df.loc[mask, \"org_uuid\"] if \"org_uuid\" in df.columns else df.index[mask])\n",
    "        for label, mask in masks.items()\n",
    "    }\n",
    "\n",
    "    for label, ids in negatives.items():\n",
    "        print(f\"{label}: {len(ids)} negative durations\")\n",
    "\n",
    "    return negatives\n",
    "\n",
    "negative_ids = find_negative_durations(df_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "72424f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time_to_close: 44 negative durations\n",
      "time_to_ipo: 676 negative durations\n",
      "time_to_acquisition: 584 negative durations\n",
      "Kept rows: 3,886,357 / 3,887,655\n"
     ]
    }
   ],
   "source": [
    "# Drop all organizations with negative durations across close/IPO/acquisition timelines.\n",
    "\n",
    "def drop_negative_durations(df: pd.DataFrame) -> tuple[pd.DataFrame, dict[str, pd.Index]]:\n",
    "    required = [\"founded_on\", \"closed_on\", \"went_public_on\", \"acquired_on_first\"]\n",
    "    missing = [col for col in required if col not in df.columns]\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "    founded = pd.to_datetime(df[\"founded_on\"], errors=\"coerce\")\n",
    "    closed = pd.to_datetime(df[\"closed_on\"], errors=\"coerce\")\n",
    "    went_public = pd.to_datetime(df[\"went_public_on\"], errors=\"coerce\")\n",
    "    acquired = pd.to_datetime(df[\"acquired_on_first\"], errors=\"coerce\")\n",
    "\n",
    "    masks = {\n",
    "        \"time_to_close\": founded.notna() & closed.notna() & (closed < founded),\n",
    "        \"time_to_ipo\": founded.notna() & went_public.notna() & (went_public < founded),\n",
    "        \"time_to_acquisition\": founded.notna() & acquired.notna() & (acquired < founded),\n",
    "    }\n",
    "\n",
    "    negatives = {\n",
    "        label: (df.loc[mask, \"org_uuid\"] if \"org_uuid\" in df.columns else df.index[mask])\n",
    "        for label, mask in masks.items()\n",
    "    }\n",
    "\n",
    "    combined_mask = masks[\"time_to_close\"] | masks[\"time_to_ipo\"] | masks[\"time_to_acquisition\"]\n",
    "    cleaned = df.loc[~combined_mask].copy()\n",
    "\n",
    "    for label, ids in negatives.items():\n",
    "        print(f\"{label}: {len(ids)} negative durations\")\n",
    "    print(f\"Kept rows: {len(cleaned):,} / {len(df):,}\")\n",
    "\n",
    "    return cleaned, negatives\n",
    "\n",
    "df_filtered, negative_ids = drop_negative_durations(df_filtered)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec4e8f",
   "metadata": {},
   "source": [
    "# Enforcing Data Freeze and Cut-off\n",
    "\n",
    "Two study-wide constraints are applied here:\n",
    "\n",
    "1. **Founded-year window**: keep only organizations founded within the configured year range.\n",
    "2. **Data freeze**: cap all relevant event dates at `freeze_date` so that downstream analyses do not leak information past the study cut-off.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "afc6a438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1140666 entries, 15 to 3994570\n",
      "Columns: 119 entries, org_uuid to closed_on_backfilled\n",
      "dtypes: Float64(2), Int64(18), bool(1), datetime64[ns](12), string(86)\n",
      "memory usage: 1.0 GB\n"
     ]
    }
   ],
   "source": [
    "# Apply the founded-year cohort filter and enforce the study freeze date.\n",
    "# Any event timestamp later than `freeze_date` is capped to `freeze_date`.\n",
    "\n",
    "founded_ts = pd.to_datetime(df_filtered[\"founded_on\"], errors=\"coerce\")\n",
    "year_min, year_max = preproc_cfg[\"founded_year_range\"]\n",
    "df_filtered2 = df_filtered.loc[founded_ts.dt.year.between(year_min, year_max, inclusive=\"both\")].copy()\n",
    "\n",
    "cutoff = preproc_cfg[\"freeze_date\"]\n",
    "date_cols = [\n",
    "    \"created_at\",\n",
    "    \"last_funding_on\",\n",
    "    \"closed_on\",\n",
    "    \"first_funding_date\",\n",
    "    \"acquired_on_first\",\n",
    "    \"acquired_on_last\",\n",
    "    \"first_acquired_company_on\",\n",
    "    \"last_acquired_company_on\",\n",
    "    \"ipo_went_public_on\",\n",
    "    \"went_public_on\",\n",
    "    \"org_created_at\",\n",
    "    \"org_updated_at\",\n",
    "]\n",
    "\n",
    "for col in df_filtered2.columns.intersection(date_cols):\n",
    "    ts = pd.to_datetime(df_filtered2[col], errors=\"coerce\")\n",
    "    df_filtered2[col] = ts.mask(ts > cutoff, cutoff)\n",
    "\n",
    "df_filtered2.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7747f39a",
   "metadata": {},
   "source": [
    "# Filtering down to useful columns\n",
    "\n",
    "At this point the dataset is reduced to the columns required by the downstream feature construction and modeling steps.\n",
    "\n",
    "This includes identifiers, status/outcome dates, founder attributes, and the raw `funding_round_*` strings used for later parsing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "900cf266",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the final column set used for the thesis study dataset.\n",
    "# This keeps identifiers/outcomes/founder variables plus raw funding_round strings for later parsing.\n",
    "\n",
    "df_study = df_filtered2.loc[:, [\n",
    "    \"org_uuid\",\n",
    "    \"org_name\",\n",
    "    \"legal_name\",\n",
    "    \"homepage_url\",\n",
    "    \"status\",\n",
    "    \"org_country\",\n",
    "    \"org_city\",\n",
    "    \"founded_on\",\n",
    "    \"first_funding_date\",\n",
    "    \"date_of_1_million\",\n",
    "    \"weighted_time\",\n",
    "    \"funding_25pct_date\",\n",
    "    \"funding_25pct_round_number\",\n",
    "    \"first_round_size_to_total_funding\",\n",
    "    \"first_funding_investor_type\",\n",
    "    \"first_round_investor_uuid\",\n",
    "    \"first_funding_leads\",\n",
    "    \"founders_has_phd\",\n",
    "    \"founders_has_mba\",\n",
    "    \"founders_has_masters\",\n",
    "    \"founders_has_bachelors\",\n",
    "    \"founders_has_jd\",\n",
    "    \"founders_degrees\",\n",
    "    \"founders_count\",\n",
    "    \"founders_countries\",\n",
    "    \"founders_female_count\",\n",
    "    \"founders_male_count\",\n",
    "    \"founders_descriptions\",\n",
    "    \"parent_uuid\",\n",
    "    \"employee_count\",\n",
    "    \"closed_on\",\n",
    "    \"went_public_on\",\n",
    "    \"acquired_on_first\",\n",
    "    \"first_funding_raised_usd\",\n",
    "    \"total_funding_usd\",\n",
    "    \"first_funding_post_money_usd\",\n",
    "    \"num_funding_rounds\",\n",
    "    \"category_list\",\n",
    "    \"category_groups_list\",\n",
    "\n",
    "    \"funding_round_1\",\n",
    "    \"funding_round_2\",\n",
    "    \"funding_round_3\",\n",
    "    \"funding_round_4\",\n",
    "    \"funding_round_5\",\n",
    "    \"funding_round_6\",\n",
    "    \"funding_round_7\",\n",
    "    \"funding_round_8\",\n",
    "    \"funding_round_9\",\n",
    "    \"funding_round_10\",\n",
    "    \"funding_round_11\",\n",
    "    \"funding_round_12\",\n",
    "    \"funding_round_13\",\n",
    "    \"funding_round_14\",\n",
    "    \"funding_round_15\",\n",
    "    \"funding_round_16\",\n",
    "    \"funding_round_17\",\n",
    "    \"funding_round_18\",\n",
    "    \"funding_round_19\",\n",
    "    \"funding_round_20\",\n",
    "    \"funding_round_21\",\n",
    "    \"funding_round_22\",\n",
    "    \"funding_round_23\",\n",
    "    \"funding_round_24\",\n",
    "    \"funding_round_25\",\n",
    "    \"funding_round_26\",\n",
    "    \"funding_round_27\",\n",
    "    \"funding_round_28\",\n",
    "    \"funding_round_29\",\n",
    "    \"funding_round_30\",\n",
    "    \"funding_round_31\",\n",
    "    \"funding_round_32\",\n",
    "    \"funding_round_33\",\n",
    "    \"funding_round_34\",\n",
    "    \"funding_round_35\",\n",
    "    \"funding_round_36\",\n",
    "    \"funding_round_37\",\n",
    "    \"funding_round_38\",\n",
    "    \"funding_round_39\",\n",
    "    \"funding_round_40\",\n",
    "    \"funding_round_41\",\n",
    "    \"funding_round_42\",\n",
    "    \"funding_round_43\"\n",
    "]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28366f4f",
   "metadata": {},
   "source": [
    "# Parsing and validating funding-round histories\n",
    "\n",
    "Crunchbase funding rounds arrive as semi-structured strings (type/date/amount/investor UUIDs). This section:\n",
    "- Parses raw `funding_round_*` columns into a compact, chronological sequence.\n",
    "- Projects that sequence into stage-specific columns (e.g., `date_seed`, `amount_series_a`, `uuids_angel`).\n",
    "- Drops companies with impossible stage ordering (chronology sanity checks).\n",
    "\n",
    "The result is easier to use in event-time analyses (e.g., “time from seed to Series A”).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d547e19a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing funding strings...\n",
      "Mapping logical rounds to stage specific columns...\n",
      "Running comprehensive chronology sanity checks...\n",
      "  ! Found 667 rows where date_pre_seed is AFTER date_seed\n",
      "  ! Found 63 rows where date_pre_seed is AFTER date_series_a\n",
      "  ! Found 16 rows where date_pre_seed is AFTER date_series_b\n",
      "  ! Found 9 rows where date_pre_seed is AFTER date_series_c\n",
      "  ! Found 200 rows where date_seed is AFTER date_series_a\n",
      "  ! Found 46 rows where date_seed is AFTER date_series_b\n",
      "  ! Found 14 rows where date_seed is AFTER date_series_c\n",
      "  ! Found 27 rows where date_series_a is AFTER date_series_b\n",
      "  ! Found 13 rows where date_series_a is AFTER date_series_c\n",
      "  ! Found 14 rows where date_series_b is AFTER date_series_c\n",
      "Removing 948 companies with impossible timelines...\n",
      "df_study_pre_seed: 13820\n",
      "df_study_seed: 74722\n",
      "df_study_series_a: 34446\n",
      "df_study_seed_to_series_a: 14782\n",
      "df_study_angel: 17446\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "# Funding-round parsing and validation.\n",
    "# Converts semi-structured `funding_round_*` strings into:\n",
    "# - a compact chronological sequence (logical rounds)\n",
    "# - stage-specific columns like `date_seed`, `amount_series_a`, `uuids_angel`\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# ==========================================\n",
    "# 1. SETUP & DEFINITIONS\n",
    "# ==========================================\n",
    "\n",
    "ALLOWED_TYPES = {\n",
    "    'pre_seed', 'seed', 'angel',\n",
    "    'venture', \n",
    "    'series_a', 'series_b', 'series_c', 'series_d', 'series_e', \n",
    "    'series_f', 'series_g', 'series_h', 'series_i', 'series_j'\n",
    "}\n",
    "\n",
    "# Mapping types to destination prefixes\n",
    "TYPE_TO_DESTINATION_MAP = {\n",
    "    'pre_seed': 'pre_seed',\n",
    "    'angel':    'angel',\n",
    "    'seed':     'seed',\n",
    "    'series_a': 'series_a',\n",
    "    'series_b': 'series_b',\n",
    "    'series_c': 'series_c',\n",
    "    'series_d': 'series_c',\n",
    "    'series_e': 'series_c',\n",
    "    'series_f': 'series_c',\n",
    "    'series_g': 'series_c',\n",
    "    'series_h': 'series_c',\n",
    "}\n",
    "\n",
    "PARSING_PATTERN = re.compile(r\"^(.*?)\\s*\\(([\\d-]{4,10})\\)\\s*(.*)$\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. PARSING LOGIC\n",
    "# ==========================================\n",
    "\n",
    "def get_compact_sequence(row, input_cols, max_depth=10):\n",
    "    parsed_rounds = []\n",
    "    seen_types = set()\n",
    "    \n",
    "    for col in input_cols:\n",
    "        val = row[col]\n",
    "        if pd.isna(val) or val == \"\":\n",
    "            continue\n",
    "        \n",
    "        s_val = str(val).strip()\n",
    "        match = PARSING_PATTERN.match(s_val)\n",
    "        \n",
    "        if not match:\n",
    "            continue\n",
    "            \n",
    "        r_type = match.group(1).strip().lower()\n",
    "        r_date = match.group(2)\n",
    "        remainder = match.group(3).strip()\n",
    "        \n",
    "        r_amount = None\n",
    "        r_uuids = None\n",
    "        \n",
    "        if remainder:\n",
    "            parts = remainder.split(' ', 1)\n",
    "            if any(char.isdigit() for char in parts[0]):\n",
    "                r_amount = parts[0]\n",
    "                r_uuids = parts[1] if len(parts) > 1 else None\n",
    "            else:\n",
    "                r_amount = None\n",
    "                r_uuids = remainder\n",
    "        \n",
    "        if r_type in ALLOWED_TYPES:\n",
    "            parsed_rounds.append({\n",
    "                'type': r_type,\n",
    "                'date': r_date,\n",
    "                'amount': r_amount,\n",
    "                'uuids': r_uuids\n",
    "            })\n",
    "\n",
    "    # Sort Chronologically\n",
    "    parsed_rounds.sort(key=lambda x: x['date'])\n",
    "    \n",
    "    flat_data = []\n",
    "    for r in parsed_rounds:\n",
    "        if r['type'] not in seen_types:\n",
    "            flat_data.extend([r['type'], r['date'], r['amount'], r['uuids']])\n",
    "            seen_types.add(r['type'])\n",
    "            if len(seen_types) >= max_depth:\n",
    "                break\n",
    "            \n",
    "    # Padding\n",
    "    rounds_found = len(seen_types)\n",
    "    if rounds_found < max_depth:\n",
    "        flat_data += [None, None, None, None] * (max_depth - rounds_found)\n",
    "        \n",
    "    return flat_data\n",
    "\n",
    "# ==========================================\n",
    "# 3. APPLY TO DATAFRAME\n",
    "# ==========================================\n",
    "\n",
    "raw_funding_cols = [c for c in df_study.columns if c.startswith(\"funding_round_\")]\n",
    "# Ensure chronological inspection (though logic sorts anyway)\n",
    "raw_funding_cols.sort(key=lambda x: int(x.split('_')[-1]) if '_' in x and x.split('_')[-1].isdigit() else 0)\n",
    "\n",
    "MAX_LOGICAL_STEPS = 10 \n",
    "compact_col_names = []\n",
    "for i in range(MAX_LOGICAL_STEPS):\n",
    "    compact_col_names.extend([\n",
    "        f\"logical_round_{i+1}\", \n",
    "        f\"logical_round_{i+1}_date\", \n",
    "        f\"logical_round_{i+1}_amount\",\n",
    "        f\"logical_round_{i+1}_uuids\"\n",
    "    ])\n",
    "\n",
    "print(\"Parsing funding strings...\")\n",
    "compact_data = df_study.apply(\n",
    "    lambda row: get_compact_sequence(row, raw_funding_cols, MAX_LOGICAL_STEPS), \n",
    "    axis=1, \n",
    "    result_type='expand'\n",
    ")\n",
    "compact_data.columns = compact_col_names\n",
    "\n",
    "cols_to_drop = [c for c in df_study.columns if c.startswith(\"logical_round_\") or c.startswith(\"funding_round_\")]\n",
    "df_study = pd.concat([df_study.drop(columns=cols_to_drop, errors='ignore'), compact_data], axis=1)\n",
    "\n",
    "# ==========================================\n",
    "# 4. CREATE STAGE-SPECIFIC COLUMNS\n",
    "# ==========================================\n",
    "print(\"Mapping logical rounds to stage specific columns...\")\n",
    "\n",
    "unique_stages = set(TYPE_TO_DESTINATION_MAP.values())\n",
    "for stage in unique_stages:\n",
    "    df_study[f\"date_{stage}\"] = pd.NaT\n",
    "    df_study[f\"amount_{stage}\"] = None\n",
    "    df_study[f\"uuids_{stage}\"] = None\n",
    "\n",
    "for i in range(1, MAX_LOGICAL_STEPS + 1):\n",
    "    log_type = f\"logical_round_{i}\"\n",
    "    log_date = f\"logical_round_{i}_date\"\n",
    "    log_amt  = f\"logical_round_{i}_amount\"\n",
    "    log_uuid = f\"logical_round_{i}_uuids\"\n",
    "    \n",
    "    for src_type, target_suffix in TYPE_TO_DESTINATION_MAP.items():\n",
    "        target_date_col = f\"date_{target_suffix}\"\n",
    "        target_amt_col  = f\"amount_{target_suffix}\"\n",
    "        target_uuid_col = f\"uuids_{target_suffix}\"\n",
    "        \n",
    "        mask = (df_study[log_type] == src_type) & (df_study[target_date_col].isna())\n",
    "        \n",
    "        if mask.any():\n",
    "            df_study.loc[mask, target_date_col] = df_study.loc[mask, log_date]\n",
    "            df_study.loc[mask, target_amt_col]  = df_study.loc[mask, log_amt]\n",
    "            df_study.loc[mask, target_uuid_col] = df_study.loc[mask, log_uuid]\n",
    "\n",
    "date_cols = [f\"date_{s}\" for s in unique_stages]\n",
    "for col in date_cols:\n",
    "    df_study[col] = pd.to_datetime(df_study[col], errors='coerce')\n",
    "\n",
    "# ==========================================\n",
    "# 6. CHRONOLOGY SANITY CHECK (ROBUST)\n",
    "# ==========================================\n",
    "print(\"Running comprehensive chronology sanity checks...\")\n",
    "\n",
    "# Define the strict chronological order\n",
    "stage_order = [\n",
    "    'date_pre_seed', \n",
    "    'date_seed', \n",
    "    'date_series_a', \n",
    "    'date_series_b', \n",
    "    'date_series_c'\n",
    "]\n",
    "\n",
    "df_study['chronology_valid'] = True\n",
    "\n",
    "# Nested Loop: Compare EVERY stage against ALL SUBSEQUENT stages\n",
    "# Example: Check Pre-Seed vs Seed, Pre-Seed vs Series A, Pre-Seed vs Series B...\n",
    "for i in range(len(stage_order)):\n",
    "    early_stage = stage_order[i]\n",
    "    \n",
    "    for j in range(i + 1, len(stage_order)):\n",
    "        late_stage = stage_order[j]\n",
    "        \n",
    "        if early_stage not in df_study.columns or late_stage not in df_study.columns:\n",
    "            continue\n",
    "            \n",
    "        # Error if: Both exist AND Early Date > Late Date\n",
    "        mask_error = (\n",
    "            df_study[early_stage].notna() & \n",
    "            df_study[late_stage].notna() & \n",
    "            (df_study[early_stage] > df_study[late_stage])\n",
    "        )\n",
    "        \n",
    "        error_count = mask_error.sum()\n",
    "        if error_count > 0:\n",
    "            print(f\"  ! Found {error_count} rows where {early_stage} is AFTER {late_stage}\")\n",
    "            df_study.loc[mask_error, 'chronology_valid'] = False\n",
    "\n",
    "rows_to_drop = (~df_study['chronology_valid']).sum()\n",
    "\n",
    "if rows_to_drop > 0:\n",
    "    print(f\"Removing {rows_to_drop} companies with impossible timelines...\")\n",
    "    df_study = df_study[df_study['chronology_valid']].copy()\n",
    "    df_study.drop(columns=['chronology_valid'], inplace=True)\n",
    "else:\n",
    "    print(\"  - No chronology errors found.\")\n",
    "\n",
    "# ==========================================\n",
    "# 7. EXPORT SUBSETS\n",
    "# ==========================================\n",
    "\n",
    "base_name = \"study\" \n",
    "\n",
    "# 1. Pre-Seed Only\n",
    "df_pre_seed = df_study[df_study['date_pre_seed'].notna()].copy()\n",
    "print(f\"df_{base_name}_pre_seed: {len(df_pre_seed)}\")\n",
    "\n",
    "# 2. Seed Only\n",
    "df_seed = df_study[df_study['date_seed'].notna()].copy()\n",
    "print(f\"df_{base_name}_seed: {len(df_seed)}\")\n",
    "\n",
    "# 3. Series A Only\n",
    "df_series_a = df_study[df_study['date_series_a'].notna()].copy()\n",
    "print(f\"df_{base_name}_series_a: {len(df_series_a)}\")\n",
    "\n",
    "# 4. Seed to Series A (Both exist)\n",
    "df_seed_to_a = df_study[df_study['date_seed'].notna() & df_study['date_series_a'].notna()].copy()\n",
    "print(f\"df_{base_name}_seed_to_series_a: {len(df_seed_to_a)}\")\n",
    "\n",
    "# 5. Angel Only\n",
    "df_angel = df_study[df_study['date_angel'].notna()].copy()\n",
    "print(f\"df_{base_name}_angel: {len(df_angel)}\")\n",
    "\n",
    "print(\"Processing complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15923ed",
   "metadata": {},
   "source": [
    "# Exporting Global, UK, and US subsets\n",
    "\n",
    "After cleaning, the notebook adds a coarse `region` (continent-level) and exports:\n",
    "- the full cleaned dataset (`global_companies.csv`)\n",
    "- a UK-only subset (`org_country == 'GBR'`)\n",
    "- a US-only subset (`org_country == 'USA'`)\n",
    "\n",
    "It also prints basic `info()` summaries and counts of key statuses for quick sanity checks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72b4f8e",
   "metadata": {},
   "source": [
    "### Fixing Country Codes / Regions\n",
    "\n",
    "Crunchbase country codes are stored as ISO **alpha-3** codes (e.g., `GBR`, `USA`). For some analyses it is useful to have a broader region label.\n",
    "\n",
    "This cell maps ISO alpha-3 codes → ISO alpha-2 → continent code, using `pycountry_convert`, with a small override table for legacy codes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c3b048c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a continent-level `region` variable from ISO alpha-3 country codes.\n",
    "\n",
    "_continent_names = {\n",
    "    \"AF\": \"Africa\",\n",
    "    \"AN\": \"Antarctica\",\n",
    "    \"AS\": \"Asia\",\n",
    "    \"EU\": \"Europe\",\n",
    "    \"NA\": \"North America\",\n",
    "    \"OC\": \"Oceania\",\n",
    "    \"SA\": \"South America\",\n",
    "}\n",
    "\n",
    "_alpha3_overrides = {\n",
    "    \"ROM\": \"ROU\",  # legacy ISO code for Romania\n",
    "    \"TAN\": \"TZA\",  # legacy code for Tanzania\n",
    "}\n",
    "\n",
    "def alpha3_to_region(alpha3):\n",
    "    if not isinstance(alpha3, str) or not alpha3:\n",
    "        return pd.NA\n",
    "    alpha3 = _alpha3_overrides.get(alpha3.upper(), alpha3.upper())\n",
    "    try:\n",
    "        alpha2 = pc.country_alpha3_to_country_alpha2(alpha3)\n",
    "        continent_code = pc.country_alpha2_to_continent_code(alpha2)\n",
    "    except (KeyError, ValueError):\n",
    "        return \"Other\"\n",
    "    return _continent_names.get(continent_code, \"Other\")\n",
    "\n",
    "df_study[\"region\"] = df_study[\"org_country\"].map(alpha3_to_region).astype(\"category\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea4c911a",
   "metadata": {},
   "source": [
    "### Filtering UK\n",
    "\n",
    "Create the UK subset (`org_country == 'GBR'`), export it, and run quick sanity checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ab2b205e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UK subset (ISO alpha-3: GBR).\n",
    "\n",
    "uk_comps = df_study.loc[df_study[\"org_country\"].eq(\"GBR\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f6972572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the cleaned datasets to disk.\n",
    "\n",
    "df_study.to_csv(output_path_global, index=False)\n",
    "uk_comps.to_csv(output_path_uk, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9c246162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1139718 entries, 15 to 3994570\n",
      "Data columns (total 98 columns):\n",
      " #   Column                             Non-Null Count    Dtype         \n",
      "---  ------                             --------------    -----         \n",
      " 0   org_uuid                           1139718 non-null  string        \n",
      " 1   org_name                           1139711 non-null  string        \n",
      " 2   legal_name                         624300 non-null   string        \n",
      " 3   homepage_url                       1124380 non-null  string        \n",
      " 4   status                             1139718 non-null  string        \n",
      " 5   org_country                        1089957 non-null  string        \n",
      " 6   org_city                           1089957 non-null  string        \n",
      " 7   founded_on                         1139718 non-null  string        \n",
      " 8   first_funding_date                 131581 non-null   datetime64[ns]\n",
      " 9   date_of_1_million                  70990 non-null    string        \n",
      " 10  weighted_time                      118051 non-null   Float64       \n",
      " 11  funding_25pct_date                 118051 non-null   string        \n",
      " 12  funding_25pct_round_number         118051 non-null   Int64         \n",
      " 13  first_round_size_to_total_funding  86613 non-null    Float64       \n",
      " 14  first_funding_investor_type        89647 non-null    string        \n",
      " 15  first_round_investor_uuid          95016 non-null    string        \n",
      " 16  first_funding_leads                38929 non-null    string        \n",
      " 17  founders_has_phd                   161514 non-null   Int64         \n",
      " 18  founders_has_mba                   161514 non-null   Int64         \n",
      " 19  founders_has_masters               161514 non-null   Int64         \n",
      " 20  founders_has_bachelors             161514 non-null   Int64         \n",
      " 21  founders_has_jd                    161514 non-null   Int64         \n",
      " 22  founders_degrees                   161514 non-null   string        \n",
      " 23  founders_count                     290543 non-null   Int64         \n",
      " 24  founders_countries                 263481 non-null   string        \n",
      " 25  founders_female_count              290543 non-null   Int64         \n",
      " 26  founders_male_count                290543 non-null   Int64         \n",
      " 27  founders_descriptions              242898 non-null   string        \n",
      " 28  parent_uuid                        6824 non-null     string        \n",
      " 29  employee_count                     1139718 non-null  string        \n",
      " 30  closed_on                          31374 non-null    datetime64[ns]\n",
      " 31  went_public_on                     9123 non-null     datetime64[ns]\n",
      " 32  acquired_on_first                  46987 non-null    datetime64[ns]\n",
      " 33  first_funding_raised_usd           86626 non-null    Int64         \n",
      " 34  total_funding_usd                  118072 non-null   Int64         \n",
      " 35  first_funding_post_money_usd       6945 non-null     Int64         \n",
      " 36  num_funding_rounds                 162989 non-null   Int64         \n",
      " 37  category_list                      1110884 non-null  string        \n",
      " 38  category_groups_list               1110884 non-null  string        \n",
      " 39  logical_round_1                    112861 non-null   object        \n",
      " 40  logical_round_1_date               112861 non-null   object        \n",
      " 41  logical_round_1_amount             112861 non-null   object        \n",
      " 42  logical_round_1_uuids              82252 non-null    object        \n",
      " 43  logical_round_2                    34604 non-null    object        \n",
      " 44  logical_round_2_date               34604 non-null    object        \n",
      " 45  logical_round_2_amount             34604 non-null    object        \n",
      " 46  logical_round_2_uuids              29276 non-null    object        \n",
      " 47  logical_round_3                    13047 non-null    object        \n",
      " 48  logical_round_3_date               13047 non-null    object        \n",
      " 49  logical_round_3_amount             13047 non-null    object        \n",
      " 50  logical_round_3_uuids              12090 non-null    object        \n",
      " 51  logical_round_4                    5069 non-null     object        \n",
      " 52  logical_round_4_date               5069 non-null     object        \n",
      " 53  logical_round_4_amount             5069 non-null     object        \n",
      " 54  logical_round_4_uuids              4840 non-null     object        \n",
      " 55  logical_round_5                    1991 non-null     object        \n",
      " 56  logical_round_5_date               1991 non-null     object        \n",
      " 57  logical_round_5_amount             1991 non-null     object        \n",
      " 58  logical_round_5_uuids              1911 non-null     object        \n",
      " 59  logical_round_6                    751 non-null      object        \n",
      " 60  logical_round_6_date               751 non-null      object        \n",
      " 61  logical_round_6_amount             751 non-null      object        \n",
      " 62  logical_round_6_uuids              732 non-null      object        \n",
      " 63  logical_round_7                    285 non-null      object        \n",
      " 64  logical_round_7_date               285 non-null      object        \n",
      " 65  logical_round_7_amount             285 non-null      object        \n",
      " 66  logical_round_7_uuids              276 non-null      object        \n",
      " 67  logical_round_8                    103 non-null      object        \n",
      " 68  logical_round_8_date               103 non-null      object        \n",
      " 69  logical_round_8_amount             103 non-null      object        \n",
      " 70  logical_round_8_uuids              101 non-null      object        \n",
      " 71  logical_round_9                    31 non-null       object        \n",
      " 72  logical_round_9_date               31 non-null       object        \n",
      " 73  logical_round_9_amount             31 non-null       object        \n",
      " 74  logical_round_9_uuids              31 non-null       object        \n",
      " 75  logical_round_10                   14 non-null       object        \n",
      " 76  logical_round_10_date              14 non-null       object        \n",
      " 77  logical_round_10_amount            14 non-null       object        \n",
      " 78  logical_round_10_uuids             14 non-null       object        \n",
      " 79  date_series_a                      34446 non-null    datetime64[ns]\n",
      " 80  amount_series_a                    34446 non-null    object        \n",
      " 81  uuids_series_a                     30358 non-null    object        \n",
      " 82  date_series_c                      7526 non-null     datetime64[ns]\n",
      " 83  amount_series_c                    7526 non-null     object        \n",
      " 84  uuids_series_c                     7075 non-null     object        \n",
      " 85  date_pre_seed                      13820 non-null    datetime64[ns]\n",
      " 86  amount_pre_seed                    13820 non-null    object        \n",
      " 87  uuids_pre_seed                     10166 non-null    object        \n",
      " 88  date_series_b                      16496 non-null    datetime64[ns]\n",
      " 89  amount_series_b                    16496 non-null    object        \n",
      " 90  uuids_series_b                     15296 non-null    object        \n",
      " 91  date_angel                         17446 non-null    datetime64[ns]\n",
      " 92  amount_angel                       17446 non-null    object        \n",
      " 93  uuids_angel                        11196 non-null    object        \n",
      " 94  date_seed                          74722 non-null    datetime64[ns]\n",
      " 95  amount_seed                        74722 non-null    object        \n",
      " 96  uuids_seed                         53330 non-null    object        \n",
      " 97  region                             1089957 non-null  category      \n",
      "dtypes: Float64(2), Int64(13), category(1), datetime64[ns](10), object(52), string(20)\n",
      "memory usage: 869.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Quick schema/memory sanity check for the full (global) study dataset.\n",
    "df_study.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1a1e8dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 83062 entries, 440 to 3992639\n",
      "Data columns (total 98 columns):\n",
      " #   Column                             Non-Null Count  Dtype         \n",
      "---  ------                             --------------  -----         \n",
      " 0   org_uuid                           83062 non-null  string        \n",
      " 1   org_name                           83062 non-null  string        \n",
      " 2   legal_name                         47041 non-null  string        \n",
      " 3   homepage_url                       82297 non-null  string        \n",
      " 4   status                             83062 non-null  string        \n",
      " 5   org_country                        83062 non-null  string        \n",
      " 6   org_city                           83062 non-null  string        \n",
      " 7   founded_on                         83062 non-null  string        \n",
      " 8   first_funding_date                 8401 non-null   datetime64[ns]\n",
      " 9   date_of_1_million                  4814 non-null   string        \n",
      " 10  weighted_time                      8452 non-null   Float64       \n",
      " 11  funding_25pct_date                 8452 non-null   string        \n",
      " 12  funding_25pct_round_number         8452 non-null   Int64         \n",
      " 13  first_round_size_to_total_funding  5514 non-null   Float64       \n",
      " 14  first_funding_investor_type        6171 non-null   string        \n",
      " 15  first_round_investor_uuid          6331 non-null   string        \n",
      " 16  first_funding_leads                3084 non-null   string        \n",
      " 17  founders_has_phd                   10808 non-null  Int64         \n",
      " 18  founders_has_mba                   10808 non-null  Int64         \n",
      " 19  founders_has_masters               10808 non-null  Int64         \n",
      " 20  founders_has_bachelors             10808 non-null  Int64         \n",
      " 21  founders_has_jd                    10808 non-null  Int64         \n",
      " 22  founders_degrees                   10808 non-null  string        \n",
      " 23  founders_count                     20830 non-null  Int64         \n",
      " 24  founders_countries                 19448 non-null  string        \n",
      " 25  founders_female_count              20830 non-null  Int64         \n",
      " 26  founders_male_count                20830 non-null  Int64         \n",
      " 27  founders_descriptions              16805 non-null  string        \n",
      " 28  parent_uuid                        465 non-null    string        \n",
      " 29  employee_count                     83062 non-null  string        \n",
      " 30  closed_on                          3206 non-null   datetime64[ns]\n",
      " 31  went_public_on                     331 non-null    datetime64[ns]\n",
      " 32  acquired_on_first                  4447 non-null   datetime64[ns]\n",
      " 33  first_funding_raised_usd           5516 non-null   Int64         \n",
      " 34  total_funding_usd                  8455 non-null   Int64         \n",
      " 35  first_funding_post_money_usd       814 non-null    Int64         \n",
      " 36  num_funding_rounds                 11239 non-null  Int64         \n",
      " 37  category_list                      81528 non-null  string        \n",
      " 38  category_groups_list               81528 non-null  string        \n",
      " 39  logical_round_1                    7056 non-null   object        \n",
      " 40  logical_round_1_date               7056 non-null   object        \n",
      " 41  logical_round_1_amount             7056 non-null   object        \n",
      " 42  logical_round_1_uuids              5149 non-null   object        \n",
      " 43  logical_round_2                    1980 non-null   object        \n",
      " 44  logical_round_2_date               1980 non-null   object        \n",
      " 45  logical_round_2_amount             1980 non-null   object        \n",
      " 46  logical_round_2_uuids              1586 non-null   object        \n",
      " 47  logical_round_3                    663 non-null    object        \n",
      " 48  logical_round_3_date               663 non-null    object        \n",
      " 49  logical_round_3_amount             663 non-null    object        \n",
      " 50  logical_round_3_uuids              612 non-null    object        \n",
      " 51  logical_round_4                    228 non-null    object        \n",
      " 52  logical_round_4_date               228 non-null    object        \n",
      " 53  logical_round_4_amount             228 non-null    object        \n",
      " 54  logical_round_4_uuids              220 non-null    object        \n",
      " 55  logical_round_5                    60 non-null     object        \n",
      " 56  logical_round_5_date               60 non-null     object        \n",
      " 57  logical_round_5_amount             60 non-null     object        \n",
      " 58  logical_round_5_uuids              59 non-null     object        \n",
      " 59  logical_round_6                    19 non-null     object        \n",
      " 60  logical_round_6_date               19 non-null     object        \n",
      " 61  logical_round_6_amount             19 non-null     object        \n",
      " 62  logical_round_6_uuids              19 non-null     object        \n",
      " 63  logical_round_7                    9 non-null      object        \n",
      " 64  logical_round_7_date               9 non-null      object        \n",
      " 65  logical_round_7_amount             9 non-null      object        \n",
      " 66  logical_round_7_uuids              8 non-null      object        \n",
      " 67  logical_round_8                    4 non-null      object        \n",
      " 68  logical_round_8_date               4 non-null      object        \n",
      " 69  logical_round_8_amount             4 non-null      object        \n",
      " 70  logical_round_8_uuids              3 non-null      object        \n",
      " 71  logical_round_9                    2 non-null      object        \n",
      " 72  logical_round_9_date               2 non-null      object        \n",
      " 73  logical_round_9_amount             2 non-null      object        \n",
      " 74  logical_round_9_uuids              2 non-null      object        \n",
      " 75  logical_round_10                   0 non-null      object        \n",
      " 76  logical_round_10_date              0 non-null      object        \n",
      " 77  logical_round_10_amount            0 non-null      object        \n",
      " 78  logical_round_10_uuids             0 non-null      object        \n",
      " 79  date_series_a                      1682 non-null   datetime64[ns]\n",
      " 80  amount_series_a                    1682 non-null   object        \n",
      " 81  uuids_series_a                     1510 non-null   object        \n",
      " 82  date_series_c                      269 non-null    datetime64[ns]\n",
      " 83  amount_series_c                    269 non-null    object        \n",
      " 84  uuids_series_c                     258 non-null    object        \n",
      " 85  date_pre_seed                      961 non-null    datetime64[ns]\n",
      " 86  amount_pre_seed                    961 non-null    object        \n",
      " 87  uuids_pre_seed                     656 non-null    object        \n",
      " 88  date_series_b                      681 non-null    datetime64[ns]\n",
      " 89  amount_series_b                    681 non-null    object        \n",
      " 90  uuids_series_b                     639 non-null    object        \n",
      " 91  date_angel                         853 non-null    datetime64[ns]\n",
      " 92  amount_angel                       853 non-null    object        \n",
      " 93  uuids_angel                        399 non-null    object        \n",
      " 94  date_seed                          5451 non-null   datetime64[ns]\n",
      " 95  amount_seed                        5451 non-null   object        \n",
      " 96  uuids_seed                         4078 non-null   object        \n",
      " 97  region                             83062 non-null  category      \n",
      "dtypes: Float64(2), Int64(13), category(1), datetime64[ns](10), object(52), string(20)\n",
      "memory usage: 63.4+ MB\n"
     ]
    }
   ],
   "source": [
    "# Quick schema/memory sanity check for the UK subset.\n",
    "uk_comps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "85dca1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPO and Acquired counts per DataFrame:\n",
      " - uk_comps: IPO=269 | Acquired=3,536\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: count IPO/acquired labels in the exported subset(s).\n",
    "\n",
    "dfs_for_status_counts = [\n",
    "    \"uk_comps\"\n",
    "]\n",
    "\n",
    "status_counts = {}\n",
    "\n",
    "for name in dfs_for_status_counts:\n",
    "    _df = globals().get(name)\n",
    "    if _df is None or \"status\" not in _df.columns:\n",
    "        continue\n",
    "\n",
    "    status_series = _df[\"status\"].astype(str).str.lower()\n",
    "    counts = status_series.value_counts()\n",
    "    status_counts[name] = {\n",
    "        \"ipo\": int(counts.get(\"ipo\", 0)),\n",
    "        \"acquired\": int(counts.get(\"acquired\", 0)),\n",
    "    }\n",
    "\n",
    "if status_counts:\n",
    "    print(\"IPO and Acquired counts per DataFrame:\")\n",
    "    for name, counts in status_counts.items():\n",
    "        print(f\" - {name}: IPO={counts['ipo']:,} | Acquired={counts['acquired']:,}\")\n",
    "else:\n",
    "    print(\"No DataFrames with a status column were found for counting.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3c23d9",
   "metadata": {},
   "source": [
    "### Filtering US\n",
    "\n",
    "Create the US subset (`org_country == 'USA'`), export it, and run quick sanity checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4f566cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# US subset (ISO alpha-3: USA).\n",
    "\n",
    "usa_comps = df_study.loc[df_study[\"org_country\"].eq(\"USA\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "38911113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export the US subset to disk.\n",
    "usa_comps.to_csv(output_path_usa, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bcd124b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 417119 entries, 15 to 3994570\n",
      "Data columns (total 98 columns):\n",
      " #   Column                             Non-Null Count   Dtype         \n",
      "---  ------                             --------------   -----         \n",
      " 0   org_uuid                           417119 non-null  string        \n",
      " 1   org_name                           417117 non-null  string        \n",
      " 2   legal_name                         242438 non-null  string        \n",
      " 3   homepage_url                       412698 non-null  string        \n",
      " 4   status                             417119 non-null  string        \n",
      " 5   org_country                        417119 non-null  string        \n",
      " 6   org_city                           417119 non-null  string        \n",
      " 7   founded_on                         417119 non-null  string        \n",
      " 8   first_funding_date                 50509 non-null   datetime64[ns]\n",
      " 9   date_of_1_million                  30904 non-null   string        \n",
      " 10  weighted_time                      49891 non-null   Float64       \n",
      " 11  funding_25pct_date                 49891 non-null   string        \n",
      " 12  funding_25pct_round_number         49891 non-null   Int64         \n",
      " 13  first_round_size_to_total_funding  36266 non-null   Float64       \n",
      " 14  first_funding_investor_type        30471 non-null   string        \n",
      " 15  first_round_investor_uuid          31229 non-null   string        \n",
      " 16  first_funding_leads                11808 non-null   string        \n",
      " 17  founders_has_phd                   80215 non-null   Int64         \n",
      " 18  founders_has_mba                   80215 non-null   Int64         \n",
      " 19  founders_has_masters               80215 non-null   Int64         \n",
      " 20  founders_has_bachelors             80215 non-null   Int64         \n",
      " 21  founders_has_jd                    80215 non-null   Int64         \n",
      " 22  founders_degrees                   80215 non-null   string        \n",
      " 23  founders_count                     125319 non-null  Int64         \n",
      " 24  founders_countries                 112558 non-null  string        \n",
      " 25  founders_female_count              125319 non-null  Int64         \n",
      " 26  founders_male_count                125319 non-null  Int64         \n",
      " 27  founders_descriptions              112268 non-null  string        \n",
      " 28  parent_uuid                        2558 non-null    string        \n",
      " 29  employee_count                     417119 non-null  string        \n",
      " 30  closed_on                          16992 non-null   datetime64[ns]\n",
      " 31  went_public_on                     2101 non-null    datetime64[ns]\n",
      " 32  acquired_on_first                  22422 non-null   datetime64[ns]\n",
      " 33  first_funding_raised_usd           36266 non-null   Int64         \n",
      " 34  total_funding_usd                  49892 non-null   Int64         \n",
      " 35  first_funding_post_money_usd       2457 non-null    Int64         \n",
      " 36  num_funding_rounds                 65239 non-null   Int64         \n",
      " 37  category_list                      409589 non-null  string        \n",
      " 38  category_groups_list               409589 non-null  string        \n",
      " 39  logical_round_1                    43497 non-null   object        \n",
      " 40  logical_round_1_date               43497 non-null   object        \n",
      " 41  logical_round_1_amount             43497 non-null   object        \n",
      " 42  logical_round_1_uuids              28647 non-null   object        \n",
      " 43  logical_round_2                    14128 non-null   object        \n",
      " 44  logical_round_2_date               14128 non-null   object        \n",
      " 45  logical_round_2_amount             14128 non-null   object        \n",
      " 46  logical_round_2_uuids              11894 non-null   object        \n",
      " 47  logical_round_3                    5880 non-null    object        \n",
      " 48  logical_round_3_date               5880 non-null    object        \n",
      " 49  logical_round_3_amount             5880 non-null    object        \n",
      " 50  logical_round_3_uuids              5465 non-null    object        \n",
      " 51  logical_round_4                    2551 non-null    object        \n",
      " 52  logical_round_4_date               2551 non-null    object        \n",
      " 53  logical_round_4_amount             2551 non-null    object        \n",
      " 54  logical_round_4_uuids              2436 non-null    object        \n",
      " 55  logical_round_5                    1115 non-null    object        \n",
      " 56  logical_round_5_date               1115 non-null    object        \n",
      " 57  logical_round_5_amount             1115 non-null    object        \n",
      " 58  logical_round_5_uuids              1076 non-null    object        \n",
      " 59  logical_round_6                    454 non-null     object        \n",
      " 60  logical_round_6_date               454 non-null     object        \n",
      " 61  logical_round_6_amount             454 non-null     object        \n",
      " 62  logical_round_6_uuids              443 non-null     object        \n",
      " 63  logical_round_7                    175 non-null     object        \n",
      " 64  logical_round_7_date               175 non-null     object        \n",
      " 65  logical_round_7_amount             175 non-null     object        \n",
      " 66  logical_round_7_uuids              172 non-null     object        \n",
      " 67  logical_round_8                    57 non-null      object        \n",
      " 68  logical_round_8_date               57 non-null      object        \n",
      " 69  logical_round_8_amount             57 non-null      object        \n",
      " 70  logical_round_8_uuids              57 non-null      object        \n",
      " 71  logical_round_9                    18 non-null      object        \n",
      " 72  logical_round_9_date               18 non-null      object        \n",
      " 73  logical_round_9_amount             18 non-null      object        \n",
      " 74  logical_round_9_uuids              18 non-null      object        \n",
      " 75  logical_round_10                   7 non-null       object        \n",
      " 76  logical_round_10_date              7 non-null       object        \n",
      " 77  logical_round_10_amount            7 non-null       object        \n",
      " 78  logical_round_10_uuids             7 non-null       object        \n",
      " 79  date_series_a                      13632 non-null   datetime64[ns]\n",
      " 80  amount_series_a                    13632 non-null   object        \n",
      " 81  uuids_series_a                     11846 non-null   object        \n",
      " 82  date_series_c                      3568 non-null    datetime64[ns]\n",
      " 83  amount_series_c                    3568 non-null    object        \n",
      " 84  uuids_series_c                     3341 non-null    object        \n",
      " 85  date_pre_seed                      5008 non-null    datetime64[ns]\n",
      " 86  amount_pre_seed                    5008 non-null    object        \n",
      " 87  uuids_pre_seed                     3639 non-null    object        \n",
      " 88  date_series_b                      7225 non-null    datetime64[ns]\n",
      " 89  amount_series_b                    7225 non-null    object        \n",
      " 90  uuids_series_b                     6612 non-null    object        \n",
      " 91  date_angel                         4015 non-null    datetime64[ns]\n",
      " 92  amount_angel                       4015 non-null    object        \n",
      " 93  uuids_angel                        1915 non-null    object        \n",
      " 94  date_seed                          31955 non-null   datetime64[ns]\n",
      " 95  amount_seed                        31955 non-null   object        \n",
      " 96  uuids_seed                         20495 non-null   object        \n",
      " 97  region                             417119 non-null  category      \n",
      "dtypes: Float64(2), Int64(13), category(1), datetime64[ns](10), object(52), string(20)\n",
      "memory usage: 318.2+ MB\n"
     ]
    }
   ],
   "source": [
    "# Quick schema/memory sanity check for the US subset.\n",
    "usa_comps.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ab453ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IPO and Acquired counts per DataFrame:\n",
      " - usa_comps: IPO=1,540 | Acquired=15,294\n"
     ]
    }
   ],
   "source": [
    "# Quick sanity check: count IPO/acquired labels in the exported subset(s).\n",
    "\n",
    "dfs_for_status_counts = [\n",
    "    \"usa_comps\"\n",
    "]\n",
    "\n",
    "status_counts = {}\n",
    "\n",
    "for name in dfs_for_status_counts:\n",
    "    _df = globals().get(name)\n",
    "    if _df is None or \"status\" not in _df.columns:\n",
    "        continue\n",
    "\n",
    "    status_series = _df[\"status\"].astype(str).str.lower()\n",
    "    counts = status_series.value_counts()\n",
    "    status_counts[name] = {\n",
    "        \"ipo\": int(counts.get(\"ipo\", 0)),\n",
    "        \"acquired\": int(counts.get(\"acquired\", 0)),\n",
    "    }\n",
    "\n",
    "if status_counts:\n",
    "    print(\"IPO and Acquired counts per DataFrame:\")\n",
    "    for name, counts in status_counts.items():\n",
    "        print(f\" - {name}: IPO={counts['ipo']:,} | Acquired={counts['acquired']:,}\")\n",
    "else:\n",
    "    print(\"No DataFrames with a status column were found for counting.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
