{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "803e515a",
   "metadata": {},
   "source": [
    "# Merge Crunchbase CSV Datasets\n",
    "\n",
    "This notebook merges multiple Crunchbase CSV exports into a single organization-level table using a **YAML configuration** that specifies\n",
    "\n",
    "- which file is the *base* table,\n",
    "- which columns to keep (to reduce memory and I/O),\n",
    "- optional renames and dtypes (to align keys across exports), and\n",
    "- a sequence of explicit joins."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0b4703",
   "metadata": {},
   "source": [
    "## Imports and Global Settings\n",
    "\n",
    "- `pandas` is used for the in-memory merge pipeline.\n",
    "- `yaml` provides a compact and auditable way to describe merge steps.\n",
    "- `duckdb` is imported (only inside the aggregate function), because aggregates are optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b91d8a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Optional, Union\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "# Third-party\n",
    "import pandas as pd\n",
    "import yaml\n",
    "\n",
    "# Pandas safety setting for interactive work: avoids many chained-assignment pitfalls.\n",
    "pd.options.mode.copy_on_write = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff00bf52",
   "metadata": {},
   "source": [
    "## CSV Reading and Join Utilities\n",
    "\n",
    "These helper functions implement a small, explicit interface for the merge pipeline:\n",
    "\n",
    "- **`read_csv_select`** centralizes `pd.read_csv(...)` so all tables are read consistently (column selection, renaming, and dtypes).\n",
    "- **`_apply_presort_dedupe`** optionally sorts and deduplicates the *right-hand* table before joining. This is useful when a join key is not strictly unique in the raw exports but we want a single, deterministic match.\n",
    "- **`apply_exact_join`** performs a standard pandas merge, but first ensures the join-key columns exist on both sides. This keeps the pipeline robust to partial exports where a key column might be absent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c02fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_select(path: Path,\n",
    "                    usecols: Optional[List[str]] = None,\n",
    "                    renames: Optional[Dict[str, str]] = None,\n",
    "                    dtype: Optional[Dict[str, str]] = None) -> pd.DataFrame:\n",
    "    \"\"\"Read a CSV table with optional schema controls.\n",
    "\n",
    "    I keep all `pd.read_csv(...)` calls behind this helper so that the merge pipeline\n",
    "    reads files consistently.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path:\n",
    "        File path to the CSV export.\n",
    "    usecols:\n",
    "        Optional list of columns to load (reduces I/O + memory for large exports).\n",
    "    renames:\n",
    "        Optional mapping applied after reading (used to align join keys, e.g. `uuid` → `org_uuid`).\n",
    "    dtype:\n",
    "        Optional dtype mapping forwarded to pandas (used to prevent mixed-type key columns).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Loaded (optionally column-filtered and renamed) DataFrame.\n",
    "    \"\"\"\n",
    "    # `low_memory=False` makes pandas process the file in one pass, which reduces\n",
    "    # dtype-guessing surprises on wide / heterogeneous Crunchbase exports.\n",
    "    kwargs: Dict[str, Any] = dict(low_memory=False)\n",
    "    if usecols is not None:\n",
    "        kwargs[\"usecols\"] = usecols\n",
    "    if dtype is not None:\n",
    "        kwargs[\"dtype\"] = dtype\n",
    "\n",
    "    df = pd.read_csv(path, **kwargs)\n",
    "\n",
    "    # Column normalization happens after reading so we can refer to consistent names downstream.\n",
    "    if renames:\n",
    "        df = df.rename(columns=renames)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def _apply_presort_dedupe(df: pd.DataFrame, join_cfg: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Apply optional sort + drop-duplicates rules defined in a join configuration.\n",
    "\n",
    "    This is mainly a *data quality / determinism* guard:\n",
    "    - sorting with a stable algorithm (`mergesort`) makes the chosen \"first\" row reproducible,\n",
    "    - deduplication can enforce one-row-per-key behavior on the right-hand table.\n",
    "    \"\"\"\n",
    "    if \"sort_by\" in join_cfg:\n",
    "        ascending: Union[bool, List[bool]] = join_cfg.get(\"ascending\", False)\n",
    "        df = df.sort_values(by=join_cfg[\"sort_by\"], ascending=ascending, kind=\"mergesort\")\n",
    "    if \"dedupe_on\" in join_cfg:\n",
    "        df = df.drop_duplicates(subset=join_cfg[\"dedupe_on\"], keep=\"first\")\n",
    "    return df\n",
    "\n",
    "\n",
    "def apply_exact_join(base: pd.DataFrame,\n",
    "                     right: pd.DataFrame,\n",
    "                     how: str,\n",
    "                     left_on: List[str],\n",
    "                     right_on: List[str],\n",
    "                     suffix: str) -> pd.DataFrame:\n",
    "    \"\"\"Perform a pandas merge with explicit key handling.\n",
    "\n",
    "    Before merging, the function ensures that all key columns exist on both sides.\n",
    "    In practice, Crunchbase exports can be incomplete or filtered; creating missing\n",
    "    key columns with `pd.NA` keeps the pipeline robust and avoids `KeyError`s.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - The join semantics are entirely governed by `how`, `left_on`, and `right_on`.\n",
    "    - `suffix` is appended to right-hand columns when a name collision occurs.\n",
    "    \"\"\"\n",
    "    for col in left_on:\n",
    "        if col not in base.columns:\n",
    "            base[col] = pd.NA\n",
    "    for col in right_on:\n",
    "        if col not in right.columns:\n",
    "            right[col] = pd.NA\n",
    "\n",
    "    return base.merge(right, how=how, left_on=left_on, right_on=right_on, suffixes=(\"\", suffix))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c840c",
   "metadata": {},
   "source": [
    "## YAML-Driven Merge Pipeline\n",
    "\n",
    "The core merge logic is defined by a YAML configuration. Conceptually, the configuration has three parts:\n",
    "\n",
    "1. **`base`**: the primary table (typically an organizations table), including optional `select`, `rename`, and `dtypes`.\n",
    "2. **`joins`**: an ordered list of joins to apply to the growing base table.\n",
    "3. **`final_order`** (optional): a preferred column ordering for readability.\n",
    "\n",
    "A minimal (illustrative) configuration looks like this:\n",
    "\n",
    "```yaml\n",
    "base:\n",
    "  path: /path/to/organizations.csv\n",
    "  select: [uuid, name, country_code, founded_on]\n",
    "  rename: {uuid: org_uuid}\n",
    "  dtypes: {uuid: string}\n",
    "\n",
    "joins:\n",
    "  - type: exact\n",
    "    path: /path/to/funding_rounds.csv\n",
    "    select: [org_uuid, raised_amount_usd, announced_on]\n",
    "    how: left\n",
    "    left_on: org_uuid\n",
    "    right_on: org_uuid\n",
    "    suffix: _fr\n",
    "    # Optional quality controls for non-unique right-hand keys:\n",
    "    sort_by: [org_uuid, announced_on]\n",
    "    ascending: [true, true]\n",
    "    dedupe_on: [org_uuid]\n",
    "\n",
    "final_order: [org_uuid, name, founded_on]\n",
    "```\n",
    "\n",
    "The join order is intentional: later joins can depend on columns produced by earlier steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad7e12e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_yaml_pipeline(config: Dict[str, Any]) -> pd.DataFrame:\n",
    "    \"\"\"Execute the YAML-driven merge pipeline and return the merged DataFrame.\"\"\"\n",
    "    base_cfg = config[\"base\"]\n",
    "\n",
    "    # 1) Load the base table (the left-most table for all subsequent joins).\n",
    "    base = read_csv_select(\n",
    "        Path(base_cfg[\"path\"]),\n",
    "        base_cfg.get(\"select\"),\n",
    "        base_cfg.get(\"rename\"),\n",
    "        base_cfg.get(\"dtypes\"),\n",
    "    )\n",
    "\n",
    "    # 2) Apply each configured join in order.\n",
    "    for join_cfg in config.get(\"joins\", []):\n",
    "        if join_cfg[\"type\"] != \"exact\":\n",
    "            raise ValueError(f\"Unsupported join type {join_cfg['type']} in this script\")\n",
    "\n",
    "        right_df = read_csv_select(\n",
    "            Path(join_cfg[\"path\"]),\n",
    "            join_cfg.get(\"select\"),\n",
    "            join_cfg.get(\"rename\"),\n",
    "            join_cfg.get(\"dtypes\"),\n",
    "        )\n",
    "\n",
    "        # Optional deterministic cleaning of the right-hand side.\n",
    "        right_df = _apply_presort_dedupe(right_df, join_cfg)\n",
    "\n",
    "        # Normalize YAML values so the merge always receives lists of key columns.\n",
    "        left_on = join_cfg[\"left_on\"] if isinstance(join_cfg[\"left_on\"], list) else [join_cfg[\"left_on\"]]\n",
    "        right_on = join_cfg[\"right_on\"] if isinstance(join_cfg[\"right_on\"], list) else [join_cfg[\"right_on\"]]\n",
    "\n",
    "        base = apply_exact_join(\n",
    "            base,\n",
    "            right_df,\n",
    "            join_cfg.get(\"how\", \"left\"),\n",
    "            left_on,\n",
    "            right_on,\n",
    "            join_cfg.get(\"suffix\", \"_r\"),\n",
    "        )\n",
    "\n",
    "    # 3) Optional: place key columns first for readability (without dropping any columns).\n",
    "    if \"final_order\" in config:\n",
    "        ordered_cols = [col for col in config[\"final_order\"] if col in base.columns]\n",
    "        remaining = [col for col in base.columns if col not in ordered_cols]\n",
    "        base = base[ordered_cols + remaining]\n",
    "\n",
    "    return base\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fb27401",
   "metadata": {},
   "source": [
    "## DuckDB Aggregate Enrichment\n",
    "\n",
    "Beyond the direct joins, I optionally compute a set of *derived* organization-level features from the raw Crunchbase tables using DuckDB.\n",
    "\n",
    "Why DuckDB here:\n",
    "- It can scan multiple CSVs via `read_csv_auto(...)` without loading everything into pandas.\n",
    "- The aggregation logic stays in SQL, which makes the feature definitions auditable.\n",
    "\n",
    "Implementation details:\n",
    "- The function checks that the required raw CSVs exist in the same directory as the base table.\n",
    "- Aggregates are computed **in batches of `org_uuid`** to reduce peak memory usage.\n",
    "- The result is merged back onto the base DataFrame via a left join on `org_uuid`.\n",
    "\n",
    "One convenience feature is that I also build a detailed per-round funding string list and then expand it into columns (`funding_round_1`, `funding_round_2`, …) for downstream modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f8c20143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DuckDB aggregate enrichment: the SQL is intentionally kept intact for reproducibility.\n",
    "# I only add comments/documentation in this notebook version.\n",
    "\n",
    "def _quote_path(path: Path) -> str:\n",
    "    \"\"\"Escape a file path so it can be safely embedded inside a DuckDB SQL string.\"\"\"\n",
    "    text = str(path)\n",
    "    return \"'\" + text.replace(\"'\", \"''\") + \"'\"\n",
    "\n",
    "def add_org_aggregates(\n",
    "    df: pd.DataFrame,\n",
    "    data_dir: Path,\n",
    "    verbose: bool = True,\n",
    "    batch_size: int = 500_000,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"Add organization-level aggregates via DuckDB (optional enrichment step).\n",
    "\n",
    "    This mirrors the scripted pipeline: rather than joining these wide, relational tables\n",
    "    in pandas, I compute organization-level features in SQL (DuckDB) and merge the results\n",
    "    back onto the base DataFrame.\n",
    "\n",
    "    Implementation notes\n",
    "    --------------------\n",
    "    - Uses `read_csv_auto(...)` to scan CSV exports directly from disk.\n",
    "    - Filters all feature engineering to the set of `org_uuid` present in `df`.\n",
    "    - Runs in batches to control memory usage on large exports.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import duckdb\n",
    "    except ImportError:\n",
    "        print(\"WARNING: duckdb not installed; skipping aggregates. Run: pip install duckdb\", file=sys.stderr)\n",
    "        return df\n",
    "\n",
    "    required = [\n",
    "        \"funding_rounds.csv\",\n",
    "        \"investments.csv\",\n",
    "        \"investors.csv\",\n",
    "        \"funds.csv\",\n",
    "        \"jobs.csv\",\n",
    "        \"people.csv\",\n",
    "        \"degrees.csv\",\n",
    "        \"people_descriptions.csv\",\n",
    "        \"acquisitions.csv\",\n",
    "        \"ipos.csv\",\n",
    "        \"organizations.csv\",\n",
    "        \"category_groups.csv\",\n",
    "    ]\n",
    "    missing = [name for name in required if not (data_dir / name).exists()]\n",
    "    if missing:\n",
    "        print(f\"WARNING: aggregate step skipped; missing files: {missing}\", file=sys.stderr)\n",
    "        return df\n",
    "\n",
    "    paths = {name: data_dir / name for name in required}\n",
    "\n",
    "    # Temp directory for DuckDB spills\n",
    "    temp_dir = (data_dir / \".duckdbtmp\").resolve()\n",
    "    temp_dir.mkdir(parents=True, exist_ok=True)\n",
    "    safe_temp = str(temp_dir).replace(\"'\", \"''\")\n",
    "\n",
    "    # Build the SQL once.\n",
    "    #\n",
    "    # The SQL is long by design: it defines all aggregates in a single audited block\n",
    "    # (funding, investors, founders/education, acquisitions/IPOs, categories, and per-round formatting).\n",
    "    SQL = f\"\"\"\n",
    "WITH leads AS (\n",
    "  SELECT i.funding_round_uuid,\n",
    "         string_agg(inv.name, ', ' ORDER BY inv.name) AS lead_investors\n",
    "  FROM read_csv_auto({_quote_path(paths['investments.csv'])}) i\n",
    "  JOIN read_csv_auto({_quote_path(paths['investors.csv'])}) inv ON inv.uuid = i.investor_uuid\n",
    "  WHERE lower(cast(i.is_lead_investor AS varchar)) IN ('true','1')\n",
    "  GROUP BY i.funding_round_uuid\n",
    "),\n",
    "\n",
    "-- Full funding rounds timeline (readable)\n",
    "fr AS (\n",
    "  SELECT fr.org_uuid,\n",
    "         string_agg(\n",
    "           printf('%s: %s (type=%s / %s) raised=%s pmv=%s investors=%s leads=%s',\n",
    "             coalesce(CAST(fr.announced_on AS TEXT), ''),\n",
    "             coalesce(fr.name, ''),\n",
    "             coalesce(fr.type, ''),\n",
    "             coalesce(fr.investment_type, ''),\n",
    "             coalesce(CAST(fr.raised_amount_usd AS TEXT), ''),\n",
    "             coalesce(CAST(fr.post_money_valuation_usd AS TEXT), ''),\n",
    "             coalesce(CAST(fr.investor_count AS TEXT), ''),\n",
    "             coalesce(leads.lead_investors, '')\n",
    "           ),\n",
    "           ' | ' ORDER BY fr.announced_on NULLS LAST\n",
    "         ) AS funding_rounds\n",
    "  FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "  LEFT JOIN leads ON leads.funding_round_uuid = fr.uuid\n",
    "  GROUP BY fr.org_uuid\n",
    "),\n",
    "\n",
    "-- Earliest funding round (any type) for context\n",
    "fr_ranked_all AS (\n",
    "  SELECT fr.*,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY fr.org_uuid\n",
    "           ORDER BY fr.announced_on ASC NULLS LAST\n",
    "         ) AS rn\n",
    "  FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "),\n",
    "fr_technical_first AS (\n",
    "  SELECT org_uuid,\n",
    "         printf('%s (%s)',\n",
    "           coalesce(NULLIF(fr.investment_type,''), coalesce(NULLIF(fr.type,''), 'unknown')),\n",
    "           coalesce(CAST(fr.announced_on AS TEXT), '')\n",
    "         ) AS first_technicaly_funding_type\n",
    "  FROM fr_ranked_all fr\n",
    "  WHERE fr.rn = 1\n",
    "),\n",
    "\n",
    "-- First (earliest dated) funding round per org, with scalar features\n",
    "-- UPDATED LOGIC: Strict exclusion list as requested.\n",
    "fr_ranked AS (\n",
    "  SELECT fr.*,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY fr.org_uuid\n",
    "           ORDER BY fr.announced_on ASC NULLS LAST\n",
    "         ) AS rn\n",
    "  FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "  WHERE lower(coalesce(fr.investment_type, fr.type, '')) NOT IN (\n",
    "      'grant',\n",
    "      'debt_financing',\n",
    "      'post_ipo_equity',\n",
    "      'post_ipo_debt',\n",
    "      'secondary_market',\n",
    "      'non_equity_assistance',\n",
    "      'undisclosed',\n",
    "      -- Newly excluded per request:\n",
    "      'convertible_note',\n",
    "      'equity_crowdfunding',\n",
    "      'private_equity',\n",
    "      'corporate_round'\n",
    "  )\n",
    "),\n",
    "fr_investor_types AS (\n",
    "  SELECT fr.org_uuid,\n",
    "         fr.uuid AS funding_round_uuid,\n",
    "         string_agg(DISTINCT inv.investor_types, ' | ' ORDER BY inv.investor_types) AS investor_types_first_round\n",
    "  FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "  JOIN read_csv_auto({_quote_path(paths['investments.csv'])}) i  ON i.funding_round_uuid = fr.uuid\n",
    "  JOIN read_csv_auto({_quote_path(paths['investors.csv'])})  inv ON inv.uuid = i.investor_uuid\n",
    "  GROUP BY fr.org_uuid, fr.uuid\n",
    "),\n",
    "fr_first AS (\n",
    "  SELECT r.org_uuid, r.uuid AS first_round_uuid,\n",
    "         r.announced_on  AS first_funding_date,\n",
    "         r.name          AS first_funding_name,\n",
    "         r.type          AS first_funding_type,\n",
    "         r.investment_type AS first_funding_investment_type,\n",
    "         r.raised_amount_usd        AS first_funding_raised_usd,\n",
    "         r.post_money_valuation_usd AS first_funding_post_money_usd,\n",
    "         r.investor_count           AS first_funding_investor_count,\n",
    "         l.lead_investors           AS first_funding_leads,\n",
    "         it_round.investor_types_first_round AS first_funding_investor_type\n",
    "  FROM fr_ranked r\n",
    "  LEFT JOIN leads l ON l.funding_round_uuid = r.uuid\n",
    "  LEFT JOIN fr_investor_types it_round ON it_round.funding_round_uuid = r.uuid\n",
    "  WHERE r.rn = 1\n",
    "),\n",
    "fr_first_investor AS (\n",
    "  SELECT fr.org_uuid,\n",
    "         fr.first_round_uuid,\n",
    "         string_agg(DISTINCT inv.uuid, ' | ' ORDER BY inv.uuid) AS first_round_investor_uuid\n",
    "  FROM fr_first fr\n",
    "  JOIN read_csv_auto({_quote_path(paths['investments.csv'])}) i ON i.funding_round_uuid = fr.first_round_uuid\n",
    "  JOIN read_csv_auto({_quote_path(paths['investors.csv'])}) inv ON inv.uuid = i.investor_uuid\n",
    "  GROUP BY fr.org_uuid, fr.first_round_uuid\n",
    "),\n",
    "\n",
    "-- Funds invested with investor_types included (dedupe first, then aggregate)\n",
    "funds_vals AS (\n",
    "  SELECT DISTINCT fr.org_uuid,\n",
    "         f.name AS fund_name,\n",
    "         printf('%s (types=%s)',\n",
    "           coalesce(f.name,''), coalesce(inv.investor_types,'')\n",
    "         ) AS val,\n",
    "         inv.investor_types\n",
    "  FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "  JOIN read_csv_auto({_quote_path(paths['investments.csv'])}) i  ON i.funding_round_uuid = fr.uuid\n",
    "  JOIN read_csv_auto({_quote_path(paths['investors.csv'])})  inv ON inv.uuid = i.investor_uuid\n",
    "  JOIN read_csv_auto({_quote_path(paths['funds.csv'])})      f   ON f.entity_uuid = inv.uuid\n",
    "),\n",
    "funds_invested AS (\n",
    "  SELECT org_uuid, string_agg(val, ' | ' ORDER BY fund_name) AS funds_invested\n",
    "  FROM funds_vals\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "investor_types_all AS (\n",
    "  SELECT org_uuid,\n",
    "         string_agg(DISTINCT investor_types, ' | ' ORDER BY investor_types) AS investor_types_all\n",
    "  FROM funds_vals\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "\n",
    "-- Organizations filtered to the base orgs (reuse for funding features and categories)\n",
    "orgs_filtered AS (\n",
    "  SELECT o.uuid AS org_uuid,\n",
    "         o.founded_on,\n",
    "         o.total_funding_usd,\n",
    "         o.category_list,\n",
    "         o.homepage_url,\n",
    "         o.created_at,\n",
    "         o.updated_at\n",
    "  FROM read_csv_auto({_quote_path(paths['organizations.csv'])}) o\n",
    "  JOIN base_orgs b ON b.org_uuid = o.uuid\n",
    "),\n",
    "\n",
    "-- Weighted average time (months) at which funding was raised\n",
    "funding_with_foundation AS (\n",
    "  SELECT fr.org_uuid,\n",
    "         fr.raised_amount_usd,\n",
    "         fr.announced_on,\n",
    "         DATE_DIFF('month', of.founded_on, fr.announced_on) AS months_from_founding\n",
    "  FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "  JOIN orgs_filtered of ON of.org_uuid = fr.org_uuid\n",
    "  WHERE fr.raised_amount_usd IS NOT NULL\n",
    "    AND fr.announced_on IS NOT NULL\n",
    "    AND of.founded_on IS NOT NULL\n",
    "),\n",
    "weighted_time AS (\n",
    "  SELECT org_uuid,\n",
    "         SUM(CAST(raised_amount_usd AS DOUBLE) * months_from_founding) / NULLIF(SUM(CAST(raised_amount_usd AS DOUBLE)), 0) AS weighted_time\n",
    "  FROM funding_with_foundation\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "\n",
    "-- Funding progression to 25% of total funding\n",
    "funding_rounds_ordered AS (\n",
    "  SELECT fr.org_uuid,\n",
    "         fr.announced_on,\n",
    "         fr.raised_amount_usd,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY fr.org_uuid\n",
    "           ORDER BY fr.announced_on ASC NULLS LAST, fr.uuid\n",
    "         ) AS round_number,\n",
    "         SUM(CAST(fr.raised_amount_usd AS DOUBLE)) OVER (\n",
    "           PARTITION BY fr.org_uuid\n",
    "           ORDER BY fr.announced_on ASC NULLS LAST, fr.uuid\n",
    "           ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW\n",
    "         ) AS cumulative_raised\n",
    "  FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "  JOIN base_orgs b ON b.org_uuid = fr.org_uuid\n",
    "  WHERE fr.raised_amount_usd IS NOT NULL\n",
    "),\n",
    "funding_25pct_ranked AS (\n",
    "  SELECT fro.org_uuid,\n",
    "         fro.announced_on,\n",
    "         fro.round_number,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY fro.org_uuid\n",
    "           ORDER BY fro.round_number\n",
    "         ) AS rn\n",
    "  FROM funding_rounds_ordered fro\n",
    "  JOIN orgs_filtered of ON of.org_uuid = fro.org_uuid\n",
    "  WHERE of.total_funding_usd IS NOT NULL\n",
    "    AND CAST(of.total_funding_usd AS DOUBLE) > 0\n",
    "    AND fro.cumulative_raised >= 0.25 * CAST(of.total_funding_usd AS DOUBLE)\n",
    "),\n",
    "funding_25pct AS (\n",
    "  SELECT org_uuid,\n",
    "         announced_on AS funding_25pct_date,\n",
    "         round_number AS funding_25pct_round_number\n",
    "  FROM funding_25pct_ranked\n",
    "  WHERE rn = 1\n",
    "),\n",
    "\n",
    "\n",
    "-- Funding date when cumulative raised crosses 1 million USD\n",
    "funding_1m_ranked AS (\n",
    "  SELECT fro.org_uuid,\n",
    "         fro.announced_on,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY fro.org_uuid\n",
    "           ORDER BY fro.round_number\n",
    "         ) AS rn\n",
    "  FROM funding_rounds_ordered fro\n",
    "  WHERE fro.cumulative_raised >= 1000000\n",
    "),\n",
    "funding_1m AS (\n",
    "  SELECT org_uuid,\n",
    "         announced_on AS date_of_1_million\n",
    "  FROM funding_1m_ranked\n",
    "  WHERE rn = 1\n",
    "),\n",
    "\n",
    "-- Founders: detect by job title / job_type; then build readable strings & features\n",
    "founders_base AS (\n",
    "  SELECT DISTINCT j.org_uuid, j.person_uuid\n",
    "  FROM read_csv_auto({_quote_path(paths['jobs.csv'])}) j\n",
    "  WHERE lower(coalesce(j.title,'')) LIKE '%%founder%%' OR lower(coalesce(j.job_type,'')) = 'founder'\n",
    "),\n",
    "founders_vals AS (\n",
    "  SELECT DISTINCT fb.org_uuid,\n",
    "         printf('%s (%s, %s/%s/%s) – %s – %s',\n",
    "           coalesce(p.name,''), coalesce(p.gender,''), coalesce(p.country_code,''), coalesce(p.state_code,''), coalesce(p.city,''),\n",
    "           coalesce(p.featured_job_title,''), coalesce(p.linkedin_url,'')\n",
    "         ) AS val\n",
    "  FROM founders_base fb\n",
    "  JOIN read_csv_auto({_quote_path(paths['people.csv'])}) p ON p.uuid = fb.person_uuid\n",
    "),\n",
    "founders AS (\n",
    "  SELECT org_uuid, string_agg(val, ' | ' ORDER BY val) AS founders\n",
    "  FROM founders_vals\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "founders_counts AS (\n",
    "  SELECT fb.org_uuid,\n",
    "         COUNT(DISTINCT fb.person_uuid) AS founders_count,\n",
    "         SUM(CASE WHEN lower(p.gender)='female' THEN 1 ELSE 0 END) AS founders_female_count,\n",
    "         SUM(CASE WHEN lower(p.gender)='male'   THEN 1 ELSE 0 END) AS founders_male_count,\n",
    "         SUM(CASE WHEN coalesce(p.linkedin_url,'') <> '' THEN 1 ELSE 0 END) AS founders_linkedin_count,\n",
    "         string_agg(DISTINCT p.country_code, ' | ' ORDER BY p.country_code) AS founders_countries\n",
    "  FROM founders_base fb\n",
    "  JOIN read_csv_auto({_quote_path(paths['people.csv'])}) p ON p.uuid = fb.person_uuid\n",
    "  GROUP BY fb.org_uuid\n",
    "),\n",
    "\n",
    "-- Best degree per founder (completed first, then latest), then degree flags\n",
    "founder_degrees_ranked AS (\n",
    "  SELECT fb.org_uuid,\n",
    "         d.person_uuid, p.name AS person_name,\n",
    "         d.name AS degree_name, d.institution_name, d.degree_type, d.subject,\n",
    "         d.is_completed, d.completed_on,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY fb.org_uuid, d.person_uuid\n",
    "           ORDER BY (CASE WHEN lower(cast(d.is_completed AS varchar)) IN ('true','1') THEN 1 ELSE 0 END) DESC,\n",
    "                    d.completed_on DESC NULLS LAST\n",
    "         ) AS rn\n",
    "  FROM founders_base fb\n",
    "  JOIN read_csv_auto({_quote_path(paths['degrees.csv'])}) d ON d.person_uuid = fb.person_uuid\n",
    "  JOIN read_csv_auto({_quote_path(paths['people.csv'])})   p ON p.uuid = d.person_uuid\n",
    "),\n",
    "founder_degrees_vals AS (\n",
    "  SELECT fdr.org_uuid,\n",
    "         fdr.person_name,\n",
    "         fdr.rn,\n",
    "         printf('%s: %s (%s, %s, %s) completed_on=%s',\n",
    "           coalesce(fdr.person_name,''),\n",
    "           coalesce(fdr.degree_name,''),\n",
    "           coalesce(fdr.institution_name,''),\n",
    "           coalesce(fdr.degree_type,''),\n",
    "           coalesce(fdr.subject,''),\n",
    "           coalesce(CAST(fdr.completed_on AS TEXT),'')\n",
    "         ) AS val\n",
    "  FROM founder_degrees_ranked fdr\n",
    "),\n",
    "founders_degrees AS (\n",
    "  SELECT org_uuid,\n",
    "         string_agg(val, ' | ' ORDER BY person_name, rn) AS founders_degrees\n",
    "  FROM founder_degrees_vals\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "degree_flags AS (\n",
    "  SELECT org_uuid,\n",
    "         MAX(CASE WHEN lower(coalesce(degree_type,'')) LIKE '%%phd%%' OR lower(coalesce(degree_name,'')) LIKE '%%phd%%'\n",
    "                   OR lower(coalesce(degree_type,'')) LIKE '%%doctor%%' OR lower(coalesce(degree_name,'')) LIKE '%%doctor%%'\n",
    "                   OR lower(coalesce(degree_name,'')) LIKE '%%doctorate%%'\n",
    "             THEN 1 ELSE 0 END) AS founders_has_phd,\n",
    "         MAX(CASE WHEN lower(coalesce(degree_type,'')) LIKE '%%mba%%' OR lower(coalesce(degree_name,'')) LIKE '%%mba%%' THEN 1 ELSE 0 END) AS founders_has_mba,\n",
    "         MAX(CASE WHEN lower(coalesce(degree_type,'')) LIKE '%%master%%' OR lower(coalesce(degree_type,'')) IN ('ms','msc','m.s.','ma','m.a','meng','m.eng')\n",
    "                       OR lower(coalesce(degree_name,'')) LIKE '%%master%%'\n",
    "                  THEN 1 ELSE 0 END) AS founders_has_masters,\n",
    "         MAX(CASE WHEN lower(coalesce(degree_type,'')) LIKE '%%bachelor%%' OR lower(coalesce(degree_type,'')) IN ('ba','b.a.','bs','b.s.','bsc','b.eng','beng')\n",
    "                       OR lower(coalesce(degree_name,'')) LIKE '%%bachelor%%'\n",
    "                  THEN 1 ELSE 0 END) AS founders_has_bachelors,\n",
    "         MAX(CASE WHEN lower(coalesce(degree_type,'')) LIKE '%%juris doctor%%'\n",
    "                       OR lower(coalesce(degree_name,'')) LIKE '%%juris doctor%%'\n",
    "                       OR lower(coalesce(degree_type,'')) = 'jd'\n",
    "                       OR lower(coalesce(degree_name,'')) = 'jd'\n",
    "                  THEN 1 ELSE 0 END) AS founders_has_jd\n",
    "  FROM founder_degrees_ranked\n",
    "  WHERE rn = 1\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "\n",
    "-- Founder descriptions (dedupe first)\n",
    "founders_desc_vals AS (\n",
    "  SELECT DISTINCT fb.org_uuid,\n",
    "         printf('%s: %s', p.name, pd.description) AS val\n",
    "  FROM founders_base fb\n",
    "  JOIN read_csv_auto({_quote_path(paths['people.csv'])}) p ON p.uuid = fb.person_uuid\n",
    "  JOIN read_csv_auto({_quote_path(paths['people_descriptions.csv'])}) pd ON pd.uuid = p.uuid\n",
    "),\n",
    "founders_descriptions AS (\n",
    "  SELECT org_uuid, string_agg(val, ' | ' ORDER BY val) AS founders_descriptions\n",
    "  FROM founders_desc_vals\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "\n",
    "-- Acquisitions (both directions)\n",
    "acq_base AS (\n",
    "  SELECT *\n",
    "  FROM read_csv_auto({_quote_path(paths['acquisitions.csv'])})\n",
    "),\n",
    "acq_by AS (\n",
    "  SELECT a.acquiree_uuid AS org_uuid,\n",
    "         string_agg(\n",
    "           printf('%s (%s USD; %s)',\n",
    "             coalesce(a.acquirer_name,''), coalesce(CAST(a.price_usd AS TEXT),''), coalesce(CAST(a.acquired_on AS TEXT),'')\n",
    "           ),\n",
    "           ' | ' ORDER BY a.acquired_on NULLS LAST\n",
    "         ) AS acquired_by\n",
    "  FROM acq_base a\n",
    "  GROUP BY a.acquiree_uuid\n",
    "),\n",
    "acq_by_dates AS (\n",
    "  SELECT a.acquiree_uuid AS org_uuid,\n",
    "         MIN(a.acquired_on) AS acquired_on_first,\n",
    "         MAX(a.acquired_on) AS acquired_on_last\n",
    "  FROM acq_base a\n",
    "  WHERE a.acquired_on IS NOT NULL\n",
    "  GROUP BY a.acquiree_uuid\n",
    "),\n",
    "acq_has AS (\n",
    "  SELECT a.acquirer_uuid AS org_uuid,\n",
    "         string_agg(\n",
    "           printf('%s (%s USD; %s)',\n",
    "             coalesce(a.acquiree_name,''), coalesce(CAST(a.price_usd AS TEXT),''), coalesce(CAST(a.acquired_on AS TEXT),'')\n",
    "           ),\n",
    "           ' | ' ORDER BY a.acquired_on NULLS LAST\n",
    "         ) AS has_acquired\n",
    "  FROM acq_base a\n",
    "  GROUP BY a.acquirer_uuid\n",
    "),\n",
    "acq_has_dates AS (\n",
    "  SELECT a.acquirer_uuid AS org_uuid,\n",
    "         MIN(a.acquired_on) AS first_acquired_company_on,\n",
    "         MAX(a.acquired_on) AS last_acquired_company_on\n",
    "  FROM acq_base a\n",
    "  WHERE a.acquired_on IS NOT NULL\n",
    "  GROUP BY a.acquirer_uuid\n",
    "),\n",
    "\n",
    "-- IPO information per organization\n",
    "ipo_ranked AS (\n",
    "  SELECT i.*,\n",
    "         ROW_NUMBER() OVER (\n",
    "           PARTITION BY i.org_uuid\n",
    "           ORDER BY i.updated_at DESC NULLS LAST,\n",
    "                    i.created_at DESC NULLS LAST\n",
    "         ) AS rn\n",
    "  FROM read_csv_auto({_quote_path(paths['ipos.csv'])}) i\n",
    "),\n",
    "ipo_info AS (\n",
    "  SELECT org_uuid,\n",
    "         went_public_on AS ipo_went_public_on,\n",
    "         created_at     AS ipo_created_at,\n",
    "         updated_at     AS ipo_updated_at\n",
    "  FROM ipo_ranked\n",
    "  WHERE rn = 1\n",
    "),\n",
    "\n",
    "-- Categories: expand org.category_list using the category catalog (category_groups.csv)\n",
    "org_core AS (\n",
    "  SELECT o.org_uuid,\n",
    "         o.category_list,\n",
    "         o.homepage_url,\n",
    "         o.created_at AS org_created_at,\n",
    "         o.updated_at AS org_updated_at\n",
    "  FROM orgs_filtered o\n",
    "),\n",
    "org_with_cats AS (\n",
    "  SELECT org_uuid, category_list\n",
    "  FROM org_core\n",
    "),\n",
    "-- explode org's comma-separated categories; alias UNNEST column as c\n",
    "org_cat_names AS (\n",
    "  SELECT owc.org_uuid, trim(c) AS category_name\n",
    "  FROM org_with_cats owc,\n",
    "       UNNEST(str_split(coalesce(owc.category_list,''), ',')) AS t(c)\n",
    "),\n",
    "org_cat_names_distinct AS (\n",
    "  SELECT DISTINCT org_uuid, category_name FROM org_cat_names\n",
    "),\n",
    "cat_catalog AS (\n",
    "  SELECT name, category_groups_list\n",
    "  FROM read_csv_auto({_quote_path(paths['category_groups.csv'])})\n",
    "),\n",
    "categories_canonical AS (\n",
    "  SELECT ocdn.org_uuid,\n",
    "         string_agg(DISTINCT c.name, ' | ' ORDER BY c.name) AS category_names_from_catalog\n",
    "  FROM org_cat_names_distinct ocdn\n",
    "  JOIN cat_catalog c ON lower(trim(c.name)) = lower(trim(ocdn.category_name))\n",
    "  GROUP BY ocdn.org_uuid\n",
    "),\n",
    "-- explode each matched category's groups; alias UNNEST column as g\n",
    "cat_groups_vals AS (\n",
    "  SELECT ocdn.org_uuid, trim(g) AS grp\n",
    "  FROM org_cat_names_distinct ocdn\n",
    "  JOIN cat_catalog c ON lower(trim(c.name)) = lower(trim(ocdn.category_name)),\n",
    "       UNNEST(str_split(coalesce(c.category_groups_list,''), ',')) AS t(g)\n",
    "),\n",
    "category_groups_from_catalog AS (\n",
    "  SELECT org_uuid,\n",
    "         string_agg(DISTINCT grp, ' | ' ORDER BY grp) AS category_groups_from_catalog\n",
    "  FROM cat_groups_vals\n",
    "  GROUP BY org_uuid\n",
    "),\n",
    "\n",
    "-- NEW LOGIC: Individual Funding Round Columns\n",
    "-- 1. Gather all investor UUIDs per round\n",
    "round_investors_uuids AS (\n",
    "    SELECT i.funding_round_uuid,\n",
    "           string_agg(i.investor_uuid, ', ' ORDER BY i.investor_uuid) AS inv_uuids\n",
    "    FROM read_csv_auto({_quote_path(paths['investments.csv'])}) i\n",
    "    GROUP BY i.funding_round_uuid\n",
    "),\n",
    "-- 2. Format rounds exactly as requested: \"investment_type (date) XXXXXUSD investor_uuids\"\n",
    "formatted_rounds_per_org AS (\n",
    "    SELECT fr.org_uuid,\n",
    "           fr.announced_on,\n",
    "           printf('%s (%s) %sUSD %s',\n",
    "               coalesce(fr.investment_type, fr.type, 'unknown'),\n",
    "               coalesce(CAST(fr.announced_on AS TEXT), 'unknown'),\n",
    "               coalesce(CAST(fr.raised_amount_usd AS TEXT), '0'),\n",
    "               coalesce(riu.inv_uuids, '')\n",
    "           ) AS round_fmt\n",
    "    FROM read_csv_auto({_quote_path(paths['funding_rounds.csv'])}) fr\n",
    "    LEFT JOIN round_investors_uuids riu ON riu.funding_round_uuid = fr.uuid\n",
    "),\n",
    "-- 3. Aggregate rounds into a LIST, ordered by date\n",
    "rounds_list_agg AS (\n",
    "    SELECT org_uuid,\n",
    "           list(round_fmt ORDER BY announced_on ASC NULLS LAST) AS detailed_rounds_list\n",
    "    FROM formatted_rounds_per_org\n",
    "    GROUP BY org_uuid\n",
    "),\n",
    "\n",
    "\n",
    "aggs AS (\n",
    "  SELECT o.org_uuid,\n",
    "         fr.funding_rounds,\n",
    "         ffirst.first_funding_date,\n",
    "         ffirst.first_funding_name,\n",
    "         ffirst.first_funding_type,\n",
    "         ffirst.first_funding_investment_type,\n",
    "         ft.first_technicaly_funding_type,\n",
    "         ffirst.first_funding_raised_usd,\n",
    "         ffirst.first_funding_post_money_usd,\n",
    "         ffirst.first_funding_investor_count,\n",
    "         fri.first_round_investor_uuid,\n",
    "         ffirst.first_funding_leads,\n",
    "         ffirst.first_funding_investor_type,\n",
    "         fi.funds_invested,\n",
    "         it.investor_types_all,\n",
    "         wt.weighted_time,\n",
    "         f25.funding_25pct_date,\n",
    "         f25.funding_25pct_round_number,\n",
    "         f1m.date_of_1_million,\n",
    "         f.founders,\n",
    "         fc.founders_count,\n",
    "         fc.founders_female_count,\n",
    "         fc.founders_male_count,\n",
    "         fc.founders_linkedin_count,\n",
    "         fc.founders_countries,\n",
    "         dflags.founders_has_phd,\n",
    "         dflags.founders_has_mba,\n",
    "         dflags.founders_has_jd,\n",
    "         dflags.founders_has_masters,\n",
    "         dflags.founders_has_bachelors,\n",
    "         fd.founders_degrees,\n",
    "         fdesc.founders_descriptions,\n",
    "         ab.acquired_by,\n",
    "         abdates.acquired_on_first,\n",
    "         abdates.acquired_on_last,\n",
    "         ah.has_acquired,\n",
    "         ahdates.first_acquired_company_on,\n",
    "         ahdates.last_acquired_company_on,\n",
    "         ipo.ipo_went_public_on,\n",
    "         ipo.ipo_created_at,\n",
    "         ipo.ipo_updated_at,\n",
    "         org.homepage_url,\n",
    "         org.org_created_at,\n",
    "         org.org_updated_at,\n",
    "         cc.category_names_from_catalog,\n",
    "         cg.category_groups_from_catalog,\n",
    "         rl.detailed_rounds_list -- Include the list column\n",
    "  FROM base_orgs o\n",
    "  LEFT JOIN fr               ON fr.org_uuid = o.org_uuid\n",
    "  LEFT JOIN fr_first ffirst  ON ffirst.org_uuid = o.org_uuid\n",
    "  LEFT JOIN fr_technical_first ft ON ft.org_uuid = o.org_uuid\n",
    "  LEFT JOIN fr_first_investor fri ON fri.org_uuid = o.org_uuid\n",
    "  LEFT JOIN funds_invested fi ON fi.org_uuid = o.org_uuid\n",
    "  LEFT JOIN investor_types_all it ON it.org_uuid = o.org_uuid\n",
    "  LEFT JOIN weighted_time wt ON wt.org_uuid = o.org_uuid\n",
    "  LEFT JOIN funding_25pct f25 ON f25.org_uuid = o.org_uuid\n",
    "  LEFT JOIN funding_1m f1m ON f1m.org_uuid = o.org_uuid\n",
    "  LEFT JOIN founders f       ON f.org_uuid  = o.org_uuid\n",
    "  LEFT JOIN founders_counts fc ON fc.org_uuid = o.org_uuid\n",
    "  LEFT JOIN degree_flags dflags ON dflags.org_uuid = o.org_uuid\n",
    "  LEFT JOIN founders_degrees fd  ON fd.org_uuid = o.org_uuid\n",
    "  LEFT JOIN founders_descriptions fdesc ON fdesc.org_uuid = o.org_uuid\n",
    "  LEFT JOIN acq_by ab        ON ab.org_uuid = o.org_uuid\n",
    "  LEFT JOIN acq_by_dates abdates ON abdates.org_uuid = o.org_uuid\n",
    "  LEFT JOIN acq_has ah       ON ah.org_uuid = o.org_uuid\n",
    "  LEFT JOIN acq_has_dates ahdates ON ahdates.org_uuid = o.org_uuid\n",
    "  LEFT JOIN ipo_info ipo     ON ipo.org_uuid = o.org_uuid\n",
    "  LEFT JOIN org_core org     ON org.org_uuid = o.org_uuid\n",
    "  LEFT JOIN categories_canonical cc ON cc.org_uuid = o.org_uuid\n",
    "  LEFT JOIN category_groups_from_catalog cg ON cg.org_uuid = o.org_uuid\n",
    "  LEFT JOIN rounds_list_agg rl ON rl.org_uuid = o.org_uuid\n",
    ")\n",
    "SELECT * FROM aggs\n",
    "\"\"\"\n",
    "\n",
    "    # --- Batching logic starts here ---\n",
    "    # We batch over distinct org IDs to keep peak memory stable on very large datasets.\n",
    "    org_ids = df[\"org_uuid\"].dropna().drop_duplicates().to_numpy()\n",
    "    if len(org_ids) == 0:\n",
    "        if verbose:\n",
    "            print(\"No org_uuid values found; skipping aggregates.\", file=sys.stderr)\n",
    "        return df\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Running DuckDB aggregates in batches of {batch_size:,}…\", file=sys.stderr)\n",
    "\n",
    "    all_aggs: List[pd.DataFrame] = []\n",
    "    total = len(org_ids)\n",
    "\n",
    "    for start in range(0, total, batch_size):\n",
    "        end = min(start + batch_size, total)\n",
    "        batch_orgs = org_ids[start:end]\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"  Batch {start // batch_size + 1}: orgs {start}–{end - 1}\", file=sys.stderr)\n",
    "\n",
    "        # New connection per batch to avoid accumulation of state/memory\n",
    "        con = duckdb.connect(\n",
    "            database=\":memory:\",\n",
    "            config={\n",
    "                \"temp_directory\": str(temp_dir),\n",
    "                \"memory_limit\": \"8GB\",\n",
    "                \"threads\": 1,\n",
    "                \"preserve_insertion_order\": \"false\",\n",
    "            },\n",
    "        )\n",
    "        con.execute(f\"PRAGMA temp_directory='{safe_temp}'\")\n",
    "        con.execute(\"PRAGMA memory_limit='8GB'\")\n",
    "        con.execute(\"PRAGMA threads=1\")\n",
    "        con.execute(\"PRAGMA preserve_insertion_order=false\")\n",
    "\n",
    "        # Register only this batch's orgs\n",
    "        base_orgs_batch = pd.DataFrame({\"org_uuid\": batch_orgs})\n",
    "        con.register(\"base_orgs\", base_orgs_batch)\n",
    "\n",
    "        batch_aggs = con.execute(SQL).df()\n",
    "        all_aggs.append(batch_aggs)\n",
    "\n",
    "        con.close()\n",
    "\n",
    "    aggregates = pd.concat(all_aggs, ignore_index=True)\n",
    "\n",
    "    # --- NEW: EXPLODE THE LIST COLUMN INTO SEPARATE COLUMNS ---\n",
    "    if \"detailed_rounds_list\" in aggregates.columns:\n",
    "        # 1. Clean data: Handle Python Lists OR Numpy Arrays (often returned by DuckDB)\n",
    "        def to_list_safe(x):\n",
    "            if isinstance(x, list):\n",
    "                return x\n",
    "            if hasattr(x, 'tolist'):  # Checks for numpy arrays without importing numpy\n",
    "                return x.tolist()\n",
    "            return [] # Fallback for None/NaN\n",
    "\n",
    "        cleaned_rounds = [to_list_safe(x) for x in aggregates[\"detailed_rounds_list\"]]\n",
    "\n",
    "        # 2. Create the expanded DataFrame\n",
    "        rounds_expanded = pd.DataFrame(cleaned_rounds, index=aggregates.index)\n",
    "        \n",
    "        # 3. Rename columns to funding_round_1, funding_round_2, etc.\n",
    "        rounds_expanded.columns = [f\"funding_round_{i+1}\" for i in range(rounds_expanded.shape[1])]\n",
    "        \n",
    "        # 4. Concatenate back to aggregates and drop the temporary list column\n",
    "        aggregates = pd.concat([aggregates, rounds_expanded], axis=1)\n",
    "        aggregates = aggregates.drop(columns=[\"detailed_rounds_list\"])\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Aggregates rows: {len(aggregates):,}\", file=sys.stderr)\n",
    "\n",
    "    return df.merge(aggregates, on=\"org_uuid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75fe47f",
   "metadata": {},
   "source": [
    "## High-Level Merge Wrapper\n",
    "\n",
    "`merge_crunchbase_data(...)`:\n",
    "\n",
    "- reads the YAML config from disk,\n",
    "- runs the YAML-driven merge pipeline,\n",
    "- optionally runs the DuckDB aggregate enrichment,\n",
    "- optionally writes the merged table to a CSV file.\n",
    "\n",
    "I also compute a small derived ratio feature (`first_round_size_to_total_funding`) when both ingredients are present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "16ed30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_crunchbase_data(config_path: Union[str, Path],\n",
    "                          output_path: Optional[Union[str, Path]] = None,\n",
    "                          sep: str = \",\",\n",
    "                          skip_aggs: bool = False,\n",
    "                          verbose: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"Merge Crunchbase CSV exports using the YAML config and optionally write the merged CSV.\"\"\"\n",
    "    config_path = Path(config_path)\n",
    "\n",
    "    # Load merge specification from disk (explicit, versionable, and auditable).\n",
    "    with config_path.open(\"r\", encoding=\"utf-8\") as handle:\n",
    "        config = yaml.safe_load(handle)\n",
    "\n",
    "    # Apply the declarative join pipeline.\n",
    "    df = run_yaml_pipeline(config)\n",
    "\n",
    "    # Optional feature enrichment using DuckDB.\n",
    "    if not skip_aggs:\n",
    "        data_dir = Path(config[\"base\"][\"path\"]).resolve().parent\n",
    "        df = add_org_aggregates(df, data_dir, verbose=verbose)\n",
    "\n",
    "    # Derived ratio: scale of the first funding round relative to total funding.\n",
    "    if {\"first_funding_raised_usd\", \"total_funding_usd\"}.issubset(df.columns):\n",
    "        first_round = pd.to_numeric(df[\"first_funding_raised_usd\"], errors=\"coerce\")\n",
    "        total_funding = pd.to_numeric(df[\"total_funding_usd\"], errors=\"coerce\")\n",
    "        ratio = first_round / total_funding\n",
    "        df[\"first_round_size_to_total_funding\"] = ratio.where(total_funding > 0)\n",
    "\n",
    "    # Optional: apply a stable, readable column ordering.\n",
    "    if \"final_order\" in config:\n",
    "        ordered_cols = [col for col in config[\"final_order\"] if col in df.columns]\n",
    "        remaining = [col for col in df.columns if col not in ordered_cols]\n",
    "        df = df[ordered_cols + remaining]\n",
    "\n",
    "    # Optional output writing (kept separate so the function can be used interactively).\n",
    "    if output_path is not None:\n",
    "        output_path = Path(output_path)\n",
    "        df.to_csv(output_path, index=False, quoting=csv.QUOTE_MINIMAL, sep=sep)\n",
    "        if verbose:\n",
    "            print(\n",
    "                f\"Wrote {output_path} with {len(df):,} rows and {len(df.columns)} columns.\",\n",
    "                file=sys.stderr,\n",
    "            )\n",
    "\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2004d9",
   "metadata": {},
   "source": [
    "## Example Usage\n",
    "\n",
    "The cell below shows the typical workflow:\n",
    "\n",
    "1. Point `config_path` to the YAML configuration you want to run.\n",
    "2. Set `output_csv` to a filename (or `None` to skip writing).\n",
    "3. Run the merge and inspect the head of the resulting DataFrame.\n",
    "\n",
    "If you want a quick test without DuckDB, set `skip_aggs=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dfebf12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running DuckDB aggregates in batches of 500,000…\n",
      "  Batch 1: orgs 0–499999\n",
      "  Batch 2: orgs 500000–999999\n",
      "  Batch 3: orgs 1000000–1499999\n",
      "  Batch 4: orgs 1500000–1999999\n",
      "  Batch 5: orgs 2000000–2499999\n",
      "  Batch 6: orgs 2500000–2999999\n",
      "  Batch 7: orgs 3000000–3499999\n",
      "  Batch 8: orgs 3500000–3992261\n",
      "Aggregates rows: 3,992,262\n",
      "Wrote merged_big_17_12.csv with 3,994,580 rows and 118 columns.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>org_uuid</th>\n",
       "      <th>org_name</th>\n",
       "      <th>legal_name</th>\n",
       "      <th>status</th>\n",
       "      <th>founded_on</th>\n",
       "      <th>first_funding_date</th>\n",
       "      <th>last_funding_on</th>\n",
       "      <th>closed_on</th>\n",
       "      <th>went_public_on</th>\n",
       "      <th>acquired_on_first</th>\n",
       "      <th>...</th>\n",
       "      <th>funding_round_34</th>\n",
       "      <th>funding_round_35</th>\n",
       "      <th>funding_round_36</th>\n",
       "      <th>funding_round_37</th>\n",
       "      <th>funding_round_38</th>\n",
       "      <th>funding_round_39</th>\n",
       "      <th>funding_round_40</th>\n",
       "      <th>funding_round_41</th>\n",
       "      <th>funding_round_42</th>\n",
       "      <th>funding_round_43</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>e1393508-30ea-8a36-3f96-dd3226033abd</td>\n",
       "      <td>Wetpaint</td>\n",
       "      <td>wetpaint.com, inc.</td>\n",
       "      <td>acquired</td>\n",
       "      <td>2005-06-01</td>\n",
       "      <td>2005-10-01</td>\n",
       "      <td>2008-05-19</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2013-12-16</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bf4d7b0e-b34d-2fd8-d292-6049c4f7efc7</td>\n",
       "      <td>Zoho</td>\n",
       "      <td>Zoho Corporation Pvt. Ltd.</td>\n",
       "      <td>operating</td>\n",
       "      <td>1996-03-17</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5f2b40b8-d1b3-d323-d81a-b7a8e89553d0</td>\n",
       "      <td>Digg</td>\n",
       "      <td>Digg Holdings, LLC</td>\n",
       "      <td>acquired</td>\n",
       "      <td>2004-10-11</td>\n",
       "      <td>2005-10-28</td>\n",
       "      <td>2016-09-13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-07-12</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f4d5ab44-058b-298b-ea81-380e6e9a8eec</td>\n",
       "      <td>Omidyar Network</td>\n",
       "      <td>Omidyar Network Services LLC</td>\n",
       "      <td>operating</td>\n",
       "      <td>2004-01-01</td>\n",
       "      <td>NaT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>df662812-7f97-0b43-9d3e-12f64f504fbb</td>\n",
       "      <td>Meta</td>\n",
       "      <td>Meta Platforms, Inc.</td>\n",
       "      <td>ipo</td>\n",
       "      <td>2004-02-04</td>\n",
       "      <td>2004-09-01</td>\n",
       "      <td>2024-11-12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012-05-18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>...</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 118 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                               org_uuid         org_name  \\\n",
       "0  e1393508-30ea-8a36-3f96-dd3226033abd         Wetpaint   \n",
       "1  bf4d7b0e-b34d-2fd8-d292-6049c4f7efc7             Zoho   \n",
       "2  5f2b40b8-d1b3-d323-d81a-b7a8e89553d0             Digg   \n",
       "3  f4d5ab44-058b-298b-ea81-380e6e9a8eec  Omidyar Network   \n",
       "4  df662812-7f97-0b43-9d3e-12f64f504fbb             Meta   \n",
       "\n",
       "                     legal_name     status  founded_on first_funding_date  \\\n",
       "0            wetpaint.com, inc.   acquired  2005-06-01         2005-10-01   \n",
       "1    Zoho Corporation Pvt. Ltd.  operating  1996-03-17                NaT   \n",
       "2            Digg Holdings, LLC   acquired  2004-10-11         2005-10-28   \n",
       "3  Omidyar Network Services LLC  operating  2004-01-01                NaT   \n",
       "4          Meta Platforms, Inc.        ipo  2004-02-04         2004-09-01   \n",
       "\n",
       "  last_funding_on closed_on went_public_on acquired_on_first  ...  \\\n",
       "0      2008-05-19       NaN            NaN        2013-12-16  ...   \n",
       "1             NaN       NaN            NaN               NaT  ...   \n",
       "2      2016-09-13       NaN            NaN        2012-07-12  ...   \n",
       "3             NaN       NaN            NaN               NaT  ...   \n",
       "4      2024-11-12       NaN     2012-05-18               NaT  ...   \n",
       "\n",
       "  funding_round_34 funding_round_35 funding_round_36 funding_round_37  \\\n",
       "0             None             None             None             None   \n",
       "1             None             None             None             None   \n",
       "2             None             None             None             None   \n",
       "3             None             None             None             None   \n",
       "4             None             None             None             None   \n",
       "\n",
       "  funding_round_38 funding_round_39 funding_round_40 funding_round_41  \\\n",
       "0             None             None             None             None   \n",
       "1             None             None             None             None   \n",
       "2             None             None             None             None   \n",
       "3             None             None             None             None   \n",
       "4             None             None             None             None   \n",
       "\n",
       "  funding_round_42 funding_round_43  \n",
       "0             None             None  \n",
       "1             None             None  \n",
       "2             None             None  \n",
       "3             None             None  \n",
       "4             None             None  \n",
       "\n",
       "[5 rows x 118 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# Point this to the YAML configuration that defines your merge (base table + joins).\n",
    "config_path = Path(\"/Users/stefan/Desktop/Thesis/v4/Crunchbase Data/Merging CB Datasets/org_big_config.yaml\")  # update as needed\n",
    "\n",
    "# Output filename is relative to the current working directory of the notebook.\n",
    "# Set to None if you only want an in-memory DataFrame.\n",
    "output_csv = Path(\"merged_big_17_12.csv\")      # set to None to skip writing\n",
    "\n",
    "# Main entry point.\n",
    "merged_df = merge_crunchbase_data(config_path, output_path=output_csv, sep=\",\", skip_aggs=False)\n",
    "merged_df.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "master-thesis-py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
